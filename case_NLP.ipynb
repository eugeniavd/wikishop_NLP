{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "toc": true,
        "id": "KQF3k_lVqDcs"
      },
      "source": [
        "<h1>Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\">\n",
        "  <ul class=\"toc-item\">\n",
        "    <li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span></li>\n",
        "    <li><span><a href=\"#Training\" data-toc-modified-id=\"Training-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Training</a></span>\n",
        "      <ul class=\"toc-item\">\n",
        "        <li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li>\n",
        "        <li><span><a href=\"#Logistic-Regression-with-Additional-Features\" data-toc-modified-id=\"Logistic-Regression-with-Additional-Features-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Logistic Regression with Additional Features</a></span></li>\n",
        "        <li><span><a href=\"#BERT\" data-toc-modified-id=\"BERT-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>BERT</a></span></li>\n",
        "        <li><span><a href=\"#Evaluating-the-Best-Model-on-the-Test-Set\" data-toc-modified-id=\"Evaluating-the-Best-Model-on-the-Test-Set-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Evaluating the Best Model on the Test Set</a></span></li>\n",
        "      </ul>\n",
        "    </li>\n",
        "    <li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Conclusions</a></span></li>\n",
        "    <li><span><a href=\"#Checklist\" data-toc-modified-id=\"Checklist-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Checklist</a></span></li>\n",
        "  </ul>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIQjqy8IgM40"
      },
      "source": [
        "# Wikishop Project with BERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8yVVR3gM40"
      },
      "source": [
        "The online store “Wikishop” is launching a new service. Now users can edit and supplement product descriptions, like in wiki communities. In other words, customers suggest edits and comment on changes made by others. The store needs a tool that will detect toxic comments and send them for moderation.\n",
        "\n",
        "Train a model to classify comments as positive or negative. You have a dataset with labels indicating the toxicity of the edits.\n",
        "\n",
        "Build a model with an F1 quality metric score of at least 0.75.\n",
        "\n",
        "**Dataset Description**\n",
        "\n",
        "The data is in the file toxic_comments.csv. The text column contains the comment text, and the toxic column is the target label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEePcbL3gM40"
      },
      "source": [
        "## Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yJ7y--wgM42",
        "outputId": "81b17452-f0fa-4d80-c6df-5ca794f5f944"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All libraries are downloaded\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import pos_tag\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from datetime import datetime\n",
        "import re\n",
        "import string\n",
        "import itertools\n",
        "import contractions\n",
        "\n",
        "import gensim\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "\n",
        "from scipy.sparse import hstack, csr_matrix\n",
        "\n",
        "import torch\n",
        "import transformers\n",
        "from tqdm import notebook\n",
        "from transformers import BertTokenizer, BertConfig, BertModel\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import gc\n",
        "import sys\n",
        "\n",
        "\n",
        "print('All libraries are downloaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "mE1sIM3CgM43",
        "outputId": "6d1c7535-114a-4ddd-98f0-1136730e798b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
              "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
              "\n",
              "   toxic  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Check if the file exists\n",
        "if os.path.exists('toxic_comments.csv'):\n",
        "    data = pd.read_csv('toxic_comments.csv')\n",
        "elif os.path.exists('datasets/toxic_comments.csv'):\n",
        "    data = pd.read_csv('datasets/toxic_comments.csv')\n",
        "else:\n",
        "    print(\"❌ File not found. Make sure it is located in the current directory or inside the 'datasets' folder.\")\n",
        "\n",
        "# Display the first few rows\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "if 'data' in locals():\n",
        "    display(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB8ydI6fgM44",
        "outputId": "4733a161-844a-4a6e-daa0-1a911d1fb101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of messages: 159292\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of messages:\", len(data))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_23sb4CgM44",
        "outputId": "0e479452-214f-4b71-f171-fe23fe4f99b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159292 entries, 0 to 159291\n",
            "Data columns (total 3 columns):\n",
            " #   Column      Non-Null Count   Dtype \n",
            "---  ------      --------------   ----- \n",
            " 0   Unnamed: 0  159292 non-null  int64 \n",
            " 1   text        159292 non-null  object\n",
            " 2   toxic       159292 non-null  int64 \n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 3.6+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f7q6I6zgM44",
        "outputId": "35f76309-3532-40aa-c75c-38846d59e418"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text length: 62871166 characters\n"
          ]
        }
      ],
      "source": [
        "# Combine all tweets into a single string to check the length of the final text\n",
        "full_text = \" \".join(list(data.text.apply(str)))\n",
        "print(\"Text length:\", len(full_text), \"characters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUrp8iO2gM44",
        "outputId": "80daf6bb-6aa7-4c59-ab6b-97822c9ffbe7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>159292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>159292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                                                                                                                                             text\n",
              "count                                                                                                                                                                                                                                                                      159292\n",
              "unique                                                                                                                                                                                                                                                                     159292\n",
              "top     Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\n",
              "freq                                                                                                                                                                                                                                                                            1"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Summary statistics for text data\n",
        "data.describe(include=object)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBCZ9cfOgM45",
        "outputId": "fcdea029-505c-4f7b-daf2-2c357ea134fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique characters: 2335\n"
          ]
        }
      ],
      "source": [
        "# Count unique characters\n",
        "\n",
        "counter = Counter(full_text)\n",
        "data_symbols = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
        "data_symbols.columns = ['symbol', 'count']\n",
        "data_symbols['unicode'] = data_symbols.symbol.apply(ord)\n",
        "\n",
        "print(\"Unique characters:\", len(data_symbols))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3l5hh85gM45",
        "outputId": "945228ab-5835-4531-8466-eeff4ece5d98"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>count</th>\n",
              "      <th>unicode</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>E</td>\n",
              "      <td>122767</td>\n",
              "      <td>69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>x</td>\n",
              "      <td>90081</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>p</td>\n",
              "      <td>964791</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>l</td>\n",
              "      <td>1925660</td>\n",
              "      <td>108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a</td>\n",
              "      <td>3778552</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2330</th>\n",
              "      <td>名</td>\n",
              "      <td>1</td>\n",
              "      <td>21517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2331</th>\n",
              "      <td>☤</td>\n",
              "      <td>1</td>\n",
              "      <td>9764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2332</th>\n",
              "      <td>⁽</td>\n",
              "      <td>1</td>\n",
              "      <td>8317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2333</th>\n",
              "      <td>⁾</td>\n",
              "      <td>1</td>\n",
              "      <td>8318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2334</th>\n",
              "      <td>ピ</td>\n",
              "      <td>1</td>\n",
              "      <td>12500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2335 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     symbol    count  unicode\n",
              "0         E   122767       69\n",
              "1         x    90081      120\n",
              "2         p   964791      112\n",
              "3         l  1925660      108\n",
              "4         a  3778552       97\n",
              "...     ...      ...      ...\n",
              "2330      名        1    21517\n",
              "2331      ☤        1     9764\n",
              "2332      ⁽        1     8317\n",
              "2333      ⁾        1     8318\n",
              "2334      ピ        1    12500\n",
              "\n",
              "[2335 rows x 3 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_symbols"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-8r44G-gM45"
      },
      "outputs": [],
      "source": [
        " # Remove extra whitespace\n",
        "def replace_whitespace(text):\n",
        "    return re.sub(r'\\s+', ' ', text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SO-XqD_zgM45",
        "outputId": "3dd31031-e585-4346-d2e6-71113e1723f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning at 2025-04-06 20:09:13.550635\n",
            "End cleaning at 2025-04-06 20:09:16.127078\n"
          ]
        }
      ],
      "source": [
        "print(f\"Start cleaning at {datetime.now()}\")\n",
        "\n",
        "data['text'] = data.text.apply(replace_whitespace)\n",
        "print(f\"End cleaning at {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1V95mJugM45",
        "outputId": "5b6ebd0c-aae1-48c9-bccc-126c1f8d1a05"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks. (talk) 21:51, January 11, 2016 (UTC)</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>\" More I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\" -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know. There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport \"</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          text  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                                     Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              D'aww! He matches this background colour I'm seemingly stuck with. Thanks. (talk) 21:51, January 11, 2016 (UTC)   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                                    Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
              "3  \" More I can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\" -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know. There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport \"   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          You, sir, are my hero. Any chance you remember what page that's on?   \n",
              "\n",
              "   toxic  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  \n",
              "3      0  \n",
              "4      0  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VmtwUwCRgM46",
        "outputId": "0f5956aa-5076-4050-b01f-69857f04e5c6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td></td>\n",
              "      <td>10706581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>e</td>\n",
              "      <td>5627184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>t</td>\n",
              "      <td>4284017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>a</td>\n",
              "      <td>3778552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>o</td>\n",
              "      <td>3729522</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   symbol     count\n",
              "9          10706581\n",
              "13      e   5627184\n",
              "6       t   4284017\n",
              "4       a   3778552\n",
              "8       o   3729522"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_symbol_counter(data, column_name):\n",
        "    full_text = \" \".join(list(data[column_name].apply(str)))\n",
        "    counter = Counter(full_text)\n",
        "    return pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
        "\n",
        "data_symbols_cleaned = get_symbol_counter(data, 'text')\n",
        "data_symbols_cleaned.columns = ['symbol', 'count']\n",
        "data_symbols_cleaned.sort_values(by='count', ascending=False, inplace=True)\n",
        "data_symbols_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB1_WBLrgM46",
        "outputId": "2badac31-6569-4220-bd62-77dd145a3303"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_symbols_cleaned.head(20).plot.bar(x='symbol', y='count', figsize=(5, 4));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s2Ry0q8gM46",
        "outputId": "0ce0b7d3-c01a-491a-aee0-dcbaeda26cc9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 32526 (\\N{CJK UNIFIED IDEOGRAPH-7F0E}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 51312 (\\N{HANGUL SYLLABLE JO}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 48124 (\\N{HANGUL SYLLABLE MIN}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 35805 (\\N{CJK UNIFIED IDEOGRAPH-8BDD}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 21205 (\\N{CJK UNIFIED IDEOGRAPH-52D5}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 39029 (\\N{CJK UNIFIED IDEOGRAPH-9875}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 40881 (\\N{CJK UNIFIED IDEOGRAPH-9FB1}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 32291 (\\N{CJK UNIFIED IDEOGRAPH-7E23}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 21214 (\\N{CJK UNIFIED IDEOGRAPH-52DE}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/events.py:82: UserWarning: Glyph 12500 (\\N{KATAKANA LETTER PI}) missing from font(s) DejaVu Sans.\n",
            "  func(*args, **kwargs)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 32526 (\\N{CJK UNIFIED IDEOGRAPH-7F0E}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51312 (\\N{HANGUL SYLLABLE JO}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 49440 (\\N{HANGUL SYLLABLE SEON}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 51064 (\\N{HANGUL SYLLABLE IN}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 48124 (\\N{HANGUL SYLLABLE MIN}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 35805 (\\N{CJK UNIFIED IDEOGRAPH-8BDD}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 21205 (\\N{CJK UNIFIED IDEOGRAPH-52D5}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 39029 (\\N{CJK UNIFIED IDEOGRAPH-9875}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 40881 (\\N{CJK UNIFIED IDEOGRAPH-9FB1}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 32291 (\\N{CJK UNIFIED IDEOGRAPH-7E23}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 21214 (\\N{CJK UNIFIED IDEOGRAPH-52DE}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n",
            "/Users/eugenia/Library/Python/3.9/lib/python/site-packages/IPython/core/pylabtools.py:152: UserWarning: Glyph 12500 (\\N{KATAKANA LETTER PI}) missing from font(s) DejaVu Sans.\n",
            "  fig.canvas.print_figure(bytes_io, **kw)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 500x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_symbols_cleaned.tail(20).plot.bar(x='symbol', y='count', figsize=(5, 4));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dGQXeZFgM46",
        "outputId": "c5ca1d17-00cf-42d1-b170-aa359104cf28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Characters outside the basic Latin alphabet: 2238\n",
            "Latin characters used: 97\n"
          ]
        }
      ],
      "source": [
        "data_symbols['unicode'] = data_symbols.symbol.apply(ord)\n",
        "over_127 = data_symbols[data_symbols.unicode > 127]\n",
        "\n",
        "print(\"Characters outside the basic Latin alphabet:\", len(over_127))\n",
        "print(\"Latin characters used:\", len(data_symbols) - len(over_127))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeI72M2DgM46",
        "outputId": "43cb6fe3-5dea-459b-8107-b54066e1602f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Non-Latin characters: !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ ·•—“‘’”–「」¤¢സംവാദമോ്കെടരʍɐ¡¿†‡↔↑↓¶½⅓⅔¼¾⅛⅜⅝⅞∞«»₳฿₵₡₢₫₯€₠₣ƒ₴₭₤ℳ₥₦№₧₰£៛₨₪৳₮₩¥♠♣♥♦²³ÁáĆćÉéÍíĹĺŃńÓóŔŕŚśÚúÝýŹźÀàÈèÌìÒòÙùÂâĈĉÊêĜĝĤĥÎîĴĵÔôŜŝÛûŴŵŶŷÄäËëÏïÖöÜüŸÿßÃãẼẽĨĩÑñÕõŨũỸỹÇçĢģĶķĻļŅņŖŗŞşŢţĐđŮůǍǎČčĎďĚěǏǐĽľŇňǑǒŘřŠšŤťǓǔŽžĀāĒēĪīŌōŪūȲȳǢǣǖǘǚǜĂăĔĕĞğĬĭŎŏŬŭĊċĖėĠġİıŻżĄąĘęĮįǪǫŲųḌḍḤḥḶḷḸḹṂṃṆṇṚṛṜṝṢṣṬṭŁłŐőŰűĿŀĦħÐðÞþŒœÆæØøÅåƏəΆάΈέΉήΊίΌόΎύΏώΑαΒβΓγΔδΕεΖζΗηΘθΙιΚκΛλΜμΝνΞξΟοΠπΡρΣσςΤτΥυΦφΧχΨψΩωАаБбВвГгҐґЃѓДдЂђЕеЁёЄєЖжЗзЅѕИиІіЇїЙйЈјКкЌќЛлЉљМмНнЊњОоПпРрСсТтЋћУуЎўФфХхЦцЧчЏџШшЩщЪъЫыЬьЭэЮюЯя̪ʈɖɟɡɢʡʔɸʃʒɕʑʂʐʝɣʁʕʜʢɦɱɳɲŋɴʋɹɻɰʙʀɾɽɫɬɮɺɭʎʟɥɧɓɗʄɠʛʘǀǃǂǁɨʉɯɪʏʊɘɵɤɚɛɜɝɞʌɔɶɑɒʰʷʲˠˤⁿˡˈˌːˑ−→™☎☓✰´♫§⋅☼連絡見学迷惑🗽レンジャー竜龙御法度└┐¨יגוּמִצְרַםָלדאֶתעֲנהבפֻח≈ἰ©♪®►内ǧɩȋ琉⇔¦⇒✉☿グ☥‎…°☏˜☺​❤☮☯😜牛岩ترجمةթարգմնույঅনুবাদ翻译თარგმნიઅનુવાદअनुवाद訳ಅನುವಾದបក្រែ번역ການແປພສभषंतरकणेهமொழிபெயர்ுఅనువాదการแปลịזשׁ״ך曹操‖Ⓣ←🍁ʞʇ✍≡√⊕⊗⁠聖や 福原信三路草日本写真家事典😅⁄（） ầㄏㄨㄤㄉㄧסӧṁ🍌ẓ会話हि्ीংলϾ✐♬♩☆龍≤♂ ⁂≠ ★步驚心電視劇∂ᛏ╟─╢雲水∇∆〈ῦ͡飞天号航服सघŦ×ț✄是吗？☻คุ―ῶἐῃᾳὶὧ�豆田みせよ松浦靜山うの『甲子夜』にえる川柳。おそらくは人知ず江戸後期平藩主・静が詠んだ句鳴かぬなしてほとぎすựứ﻿́߷♀۝҈♨ውይት♑ｗ．ＡｎｏＴａｌｋｃｍ☀୯̮د੭✆☇☄下さい✎ק¸ºحوياأخفلكنصبزقئشطىثسءعغ،ًضظُذإآ✽╫《》投稿大き世✔ט顧輝仙拉啼笑姻緣穣⚇♔㎥😊●✓四野您好，歡迎來訪英文版的維基百科雖然我們隨時你在編輯但這次執筆所使用並未達到需求準若不介意可以中感謝參加☭･∀पफ़गज➥𒁳כ肥薩線白石駅付近球磨　熊県葦北郡芦町／村黃巾既故改年為｡◕‿華民國位於東亞共和洲現有最早實行立憲制成月前被廣泛承認代表注今因要土置或政治素而通稱臺灣防部聯合美務院獨體油輸出組織′मट℥☣۩۞ὼՈտی一郎♝♚內地十八省卻只指傳統范圍即包括蒙古、西藏青海新疆台 ̃☛◅➔😂순혈주의純血義專普學職班モルットあり谢ᴀ太湖黄窣堵坡庄蝴蝶ღ頭史记姓ᴥ‍ऋʨ✈😄➨¬➪ʻ˺ム̯Ӣ÷ṗ💬ध☠Ⅱファソギョạ℠∙：¯說嗎▎ª✿リガラカ첫눈初雪永∅ツ‑ो橘桔了外生－µ¹๛ˁ〈⌊〉☘健„桜蘭高校ホスクブץஎஜஆகலைறசவữ≥很𐌴𐌹𐌲𐌰𐌿ɿὈὺὀὸ᾽ὄὅἴῖὰἱἀἠὲῷἔ∴記録メ独断偏で選틀보호문서편집요청ワシ☸屌老母ⲧⲟⲩⲱ討論ॐّ↗‹›明溪！Ᵽḟḻ南‽ṅ界┘┌📞📧頁面做啥便自己講爽就ㅂ変格言特別活‧猫ต่อยมวਸੁਖˉ╦╩उलझूँҝʼộṫ楊威利里◦■【遯卋】快乐✞″천리마군千馬강룡岡대안安구志卷吕布张邈陈登臧洪漢書劉焉袁術列ǰʿ謚敏孫済王也乙継都慕武寧ˀ१२३४५६७८९わたつっこどろポティウコアごセ忍श粿紅龜▫ہ☑அதளடேணானநீோஒஉஙஇ༆影者│‒🎄ک😉ү涛濤小刀︵紫砂泥朱缎조선인민공对话页❦❖龱✗ち麗縣☝✫ậ­ọụẹᵮ⁞働勞動労빠ǌ☃国市ま１０ヽヮﾉↄ项目绍ệửươ‪‬⚔盒͉͇̞͖̼̭͓͑̈̉ȗ̹̲ͨͮ̂̄ṙ̫̥͚̜͙͍̰̺̩̗̓ͧͩ̿ͤ̎̆ș±날짐승찜길잡이자비✌▪₂韦智✘├😢😃척뉴넘ờừ⇝ٔ✤字イキ題関係をけ方良思同じ例ば場ペ京帝れ他味醫プビュざ∧屋七墓ǝℲ祖इॆễェ❝❞✋🏼☾☽😏訣終止体仮定命令已∈ხლეইতিহস先∫◄ਜ਼ਫਰਨਾਮ总督副○洛陽長▾喷火器猛柜׀پ少ế反逆マ闘君↕☞ფპოსვ擲金Շշի☜කුරඳවත්ݣ۸ݗۻݓڰڜڬ↨۾ݟݜݭ۬۵ݡڵچロチ熱雞蛋  ≽ⱷ҅≼࿓Ḡṧथ⟲ῆἑᾶἘকম্ভটোরয়েজ👍ℓℝℍ❉犬冒険▲ؤℚ칠지도佐助⁴⁰✒上东汉手搏濟照牡壹疋牝阿吉師貢賜撰氏ニザヘ佩祝过个ѧ河ねὂ臭妈烂⇄∑ド６８公ハ物語情紙風船̠九兵衛ゃひょ愛宕◁⊝⊙梨酸钾ן̴ֵै고기소柱ᵀᴬᴸᴷ🎤ῼᵃᵏ개는때려아야맛있다य😔💜ܣܘܪܝܐヒ製薬楩僑◯ৌ‏調유헌守⨹২০১Ƭ坊陸軍官आﬂ满清倒退论郭后朝鮮ｳｨｷﾍﾟﾃﾞｱ捏造対抗⊥➜😀֑演Ӝ女神ぞ翼尻Ққ‫∗ṯ咨ǽ✭Ⓐ㊟オデ🙈🙉🙊ຄົລວຸ່ມະງທອັິດີຈຊໄຕຫຼູຣໂໜຮ占독島竹◔二历ヴエノ௹ˢ받침ㅅㄷ容डॉऔ█銀羊臼井儀陳胜吳ℱ这什么番攻撃Өө卐ố⁓声援ᶏϞ排虞沖縄旭ἡ莱芜景澄区府网站ჷگ翁‐ặẫỏắấỉảểợằもʧ珊瑚干起☢⊂┏━┓┃╔═╗║╚╝┗┛长颈鹿ſಠ益波瀾万丈伝⟨⟩∪▶Ϛŧ閩客粵越ڈ◀梁҉パ△च०खबृठʾ◥💩경상Ճᛇᚹᛟᵗ᧾塩谷栗林慧渡辺兼べ折恵ϜͰϺϷͲ隻ケ決めタサげバヌゅびふボぜゴナベへ沪滬商品名☤⁽⁾ピ\n",
            "{33: '', 34: '', 35: '', 36: '', 37: '', 38: '', 39: '', 40: '', 41: '', 42: '', 43: '', 44: '', 45: '', 46: '', 47: '', 58: '', 59: '', 60: '', 61: '', 62: '', 63: '', 64: '', 91: '', 92: '', 93: '', 94: '', 95: '', 96: '', 123: '', 124: '', 125: '', 126: '', 160: '', 183: '', 8226: '', 8212: '', 8220: '', 8216: '', 8217: '', 8221: '', 8211: '', 12300: '', 12301: '', 164: '', 162: '', 3384: '', 3330: '', 3381: '', 3390: '', 3366: '', 3374: '', 3403: '', 3405: '', 3349: '', 3398: '', 3359: '', 3376: '', 653: '', 592: '', 161: '', 191: '', 8224: '', 8225: '', 8596: '', 8593: '', 8595: '', 182: '', 189: '', 8531: '', 8532: '', 188: '', 190: '', 8539: '', 8540: '', 8541: '', 8542: '', 8734: '', 171: '', 187: '', 8371: '', 3647: '', 8373: '', 8353: '', 8354: '', 8363: '', 8367: '', 8364: '', 8352: '', 8355: '', 402: '', 8372: '', 8365: '', 8356: '', 8499: '', 8357: '', 8358: '', 8470: '', 8359: '', 8368: '', 163: '', 6107: '', 8360: '', 8362: '', 2547: '', 8366: '', 8361: '', 165: '', 9824: '', 9827: '', 9829: '', 9830: '', 178: '', 179: '', 193: '', 225: '', 262: '', 263: '', 201: '', 233: '', 205: '', 237: '', 313: '', 314: '', 323: '', 324: '', 211: '', 243: '', 340: '', 341: '', 346: '', 347: '', 218: '', 250: '', 221: '', 253: '', 377: '', 378: '', 192: '', 224: '', 200: '', 232: '', 204: '', 236: '', 210: '', 242: '', 217: '', 249: '', 194: '', 226: '', 264: '', 265: '', 202: '', 234: '', 284: '', 285: '', 292: '', 293: '', 206: '', 238: '', 308: '', 309: '', 212: '', 244: '', 348: '', 349: '', 219: '', 251: '', 372: '', 373: '', 374: '', 375: '', 196: '', 228: '', 203: '', 235: '', 207: '', 239: '', 214: '', 246: '', 220: '', 252: '', 376: '', 255: '', 223: '', 195: '', 227: '', 7868: '', 7869: '', 296: '', 297: '', 209: '', 241: '', 213: '', 245: '', 360: '', 361: '', 7928: '', 7929: '', 199: '', 231: '', 290: '', 291: '', 310: '', 311: '', 315: '', 316: '', 325: '', 326: '', 342: '', 343: '', 350: '', 351: '', 354: '', 355: '', 272: '', 273: '', 366: '', 367: '', 461: '', 462: '', 268: '', 269: '', 270: '', 271: '', 282: '', 283: '', 463: '', 464: '', 317: '', 318: '', 327: '', 328: '', 465: '', 466: '', 344: '', 345: '', 352: '', 353: '', 356: '', 357: '', 467: '', 468: '', 381: '', 382: '', 256: '', 257: '', 274: '', 275: '', 298: '', 299: '', 332: '', 333: '', 362: '', 363: '', 562: '', 563: '', 482: '', 483: '', 470: '', 472: '', 474: '', 476: '', 258: '', 259: '', 276: '', 277: '', 286: '', 287: '', 300: '', 301: '', 334: '', 335: '', 364: '', 365: '', 266: '', 267: '', 278: '', 279: '', 288: '', 289: '', 304: '', 305: '', 379: '', 380: '', 260: '', 261: '', 280: '', 281: '', 302: '', 303: '', 490: '', 491: '', 370: '', 371: '', 7692: '', 7693: '', 7716: '', 7717: '', 7734: '', 7735: '', 7736: '', 7737: '', 7746: '', 7747: '', 7750: '', 7751: '', 7770: '', 7771: '', 7772: '', 7773: '', 7778: '', 7779: '', 7788: '', 7789: '', 321: '', 322: '', 336: '', 337: '', 368: '', 369: '', 319: '', 320: '', 294: '', 295: '', 208: '', 240: '', 222: '', 254: '', 338: '', 339: '', 198: '', 230: '', 216: '', 248: '', 197: '', 229: '', 399: '', 601: '', 902: '', 940: '', 904: '', 941: '', 905: '', 942: '', 906: '', 943: '', 908: '', 972: '', 910: '', 973: '', 911: '', 974: '', 913: '', 945: '', 914: '', 946: '', 915: '', 947: '', 916: '', 948: '', 917: '', 949: '', 918: '', 950: '', 919: '', 951: '', 920: '', 952: '', 921: '', 953: '', 922: '', 954: '', 923: '', 955: '', 924: '', 956: '', 925: '', 957: '', 926: '', 958: '', 927: '', 959: '', 928: '', 960: '', 929: '', 961: '', 931: '', 963: '', 962: '', 932: '', 964: '', 933: '', 965: '', 934: '', 966: '', 935: '', 967: '', 936: '', 968: '', 937: '', 969: '', 1040: '', 1072: '', 1041: '', 1073: '', 1042: '', 1074: '', 1043: '', 1075: '', 1168: '', 1169: '', 1027: '', 1107: '', 1044: '', 1076: '', 1026: '', 1106: '', 1045: '', 1077: '', 1025: '', 1105: '', 1028: '', 1108: '', 1046: '', 1078: '', 1047: '', 1079: '', 1029: '', 1109: '', 1048: '', 1080: '', 1030: '', 1110: '', 1031: '', 1111: '', 1049: '', 1081: '', 1032: '', 1112: '', 1050: '', 1082: '', 1036: '', 1116: '', 1051: '', 1083: '', 1033: '', 1113: '', 1052: '', 1084: '', 1053: '', 1085: '', 1034: '', 1114: '', 1054: '', 1086: '', 1055: '', 1087: '', 1056: '', 1088: '', 1057: '', 1089: '', 1058: '', 1090: '', 1035: '', 1115: '', 1059: '', 1091: '', 1038: '', 1118: '', 1060: '', 1092: '', 1061: '', 1093: '', 1062: '', 1094: '', 1063: '', 1095: '', 1039: '', 1119: '', 1064: '', 1096: '', 1065: '', 1097: '', 1066: '', 1098: '', 1067: '', 1099: '', 1068: '', 1100: '', 1069: '', 1101: '', 1070: '', 1102: '', 1071: '', 1103: '', 810: '', 648: '', 598: '', 607: '', 609: '', 610: '', 673: '', 660: '', 632: '', 643: '', 658: '', 597: '', 657: '', 642: '', 656: '', 669: '', 611: '', 641: '', 661: '', 668: '', 674: '', 614: '', 625: '', 627: '', 626: '', 331: '', 628: '', 651: '', 633: '', 635: '', 624: '', 665: '', 640: '', 638: '', 637: '', 619: '', 620: '', 622: '', 634: '', 621: '', 654: '', 671: '', 613: '', 615: '', 595: '', 599: '', 644: '', 608: '', 667: '', 664: '', 448: '', 451: '', 450: '', 449: '', 616: '', 649: '', 623: '', 618: '', 655: '', 650: '', 600: '', 629: '', 612: '', 602: '', 603: '', 604: '', 605: '', 606: '', 652: '', 596: '', 630: '', 593: '', 594: '', 688: '', 695: '', 690: '', 736: '', 740: '', 8319: '', 737: '', 712: '', 716: '', 720: '', 721: '', 8722: '', 8594: '', 8482: '', 9742: '', 9747: '', 10032: '', 180: '', 9835: '', 167: '', 8901: '', 9788: '', 36899: '', 32097: '', 35211: '', 23398: '', 36855: '', 24785: '', 128509: '', 12524: '', 12531: '', 12472: '', 12515: '', 12540: '', 31452: '', 40857: '', 24481: '', 27861: '', 24230: '', 9492: '', 9488: '', 168: '', 1497: '', 1490: '', 1493: '', 1468: '', 1502: '', 1460: '', 1510: '', 1456: '', 1512: '', 1463: '', 1501: '', 1464: '', 1500: '', 1491: '', 1488: '', 1462: '', 1514: '', 1506: '', 1458: '', 1504: '', 1492: '', 1489: '', 1508: '', 1467: '', 1495: '', 8776: '', 7984: '', 169: '', 9834: '', 174: '', 9658: '', 20869: '', 487: '', 617: '', 523: '', 29705: '', 8660: '', 166: '', 8658: '', 9993: '', 9791: '', 12464: '', 9765: '', 8206: '', 8230: '', 176: '', 9743: '', 732: '', 9786: '', 8203: '', 10084: '', 9774: '', 9775: '', 128540: '', 29275: '', 23721: '', 1578: '', 1585: '', 1580: '', 1605: '', 1577: '', 1385: '', 1377: '', 1408: '', 1379: '', 1396: '', 1398: '', 1400: '', 1410: '', 1397: '', 2437: '', 2472: '', 2497: '', 2476: '', 2494: '', 2470: '', 32763: '', 35793: '', 4311: '', 4304: '', 4320: '', 4306: '', 4315: '', 4316: '', 4312: '', 2693: '', 2728: '', 2753: '', 2741: '', 2750: '', 2726: '', 2309: '', 2344: '', 2369: '', 2357: '', 2366: '', 2342: '', 35379: '', 3205: '', 3240: '', 3265: '', 3253: '', 3262: '', 3238: '', 6036: '', 6016: '', 6098: '', 6042: '', 6082: '', 48264: '', 50669: '', 3713: '', 3762: '', 3737: '', 3777: '', 3739: '', 3742: '', 3754: '', 2349: '', 2359: '', 2306: '', 2340: '', 2352: '', 2325: '', 2339: '', 2375: '', 1607: '', 2990: '', 3018: '', 2996: '', 3007: '', 2986: '', 3014: '', 2991: '', 2992: '', 3021: '', 3009: '', 3077: '', 3112: '', 3137: '', 3125: '', 3134: '', 3110: '', 3585: '', 3634: '', 3619: '', 3649: '', 3611: '', 3621: '', 7883: '', 1494: '', 1513: '', 1473: '', 1524: '', 1498: '', 26361: '', 25805: '', 8214: '', 9417: '', 8592: '', 127809: '', 670: '', 647: '', 9997: '', 8801: '', 8730: '', 8853: '', 8855: '', 8288: '', 32854: '', 12420: '', 8202: '', 31119: '', 21407: '', 20449: '', 19977: '', 36335: '', 33609: '', 26085: '', 26412: '', 20889: '', 30495: '', 23478: '', 20107: '', 20856: '', 128517: '', 8260: '', 65288: '', 65289: '', 8201: '', 7847: '', 149: '', 12559: '', 12584: '', 12580: '', 12553: '', 12583: '', 1505: '', 1255: '', 7745: '', 127820: '', 7827: '', 20250: '', 35441: '', 2361: '', 2367: '', 2381: '', 2368: '', 2434: '', 2482: '', 1022: '', 10000: '', 9836: '', 9833: '', 9734: '', 40845: '', 8804: '', 9794: '', 8194: '', 8258: '', 8800: '', 8232: '', 9733: '', 27493: '', 39514: '', 24515: '', 38651: '', 35222: '', 21127: '', 8706: '', 5839: '', 9567: '', 9472: '', 9570: '', 38642: '', 27700: '', 8711: '', 8710: '', 9001: '', 8166: '', 147: '', 148: '', 865: '', 39134: '', 22825: '', 21495: '', 33322: '', 26381: '', 2360: '', 2328: '', 358: '', 215: '', 539: '', 9988: '', 26159: '', 21527: '', 65311: '', 9787: '', 3588: '', 3640: '', 8213: '', 8182: '', 7952: '', 8131: '', 8115: '', 8054: '', 8039: '', 65533: '', 35910: '', 30000: '', 12415: '', 12379: '', 12424: '', 26494: '', 28006: '', 38748: '', 23665: '', 12358: '', 12398: '', 12302: '', 30002: '', 23376: '', 22812: '', 12303: '', 12395: '', 12360: '', 12427: '', 24029: '', 26611: '', 12290: '', 12362: '', 12381: '', 12425: '', 12367: '', 12399: '', 20154: '', 30693: '', 12378: '', 27743: '', 25144: '', 24460: '', 26399: '', 24179: '', 34281: '', 20027: '', 12539: '', 38745: '', 12364: '', 35424: '', 12435: '', 12384: '', 21477: '', 40180: '', 12363: '', 12396: '', 12394: '', 12375: '', 12390: '', 12411: '', 12392: '', 12366: '', 12377: '', 7921: '', 7913: '', 65279: '', 769: '', 2039: '', 9792: '', 1757: '', 1160: '', 9832: '', 4813: '', 4845: '', 4725: '', 9809: '', 65367: '', 65294: '', 65313: '', 65358: '', 65359: '', 65332: '', 65345: '', 65356: '', 65355: '', 65347: '', 65357: '', 9728: '', 2927: '', 814: '', 1583: '', 2669: '', 9990: '', 9735: '', 9732: '', 19979: '', 12373: '', 12356: '', 9998: '', 1511: '', 184: '', 186: '', 1581: '', 1608: '', 1610: '', 1575: '', 1571: '', 1582: '', 1601: '', 1604: '', 1603: '', 1606: '', 1589: '', 1576: '', 1586: '', 1602: '', 1574: '', 1588: '', 1591: '', 1609: '', 1579: '', 1587: '', 1569: '', 1593: '', 1594: '', 1548: '', 1611: '', 1590: '', 1592: '', 1615: '', 1584: '', 1573: '', 1570: '', 10045: '', 153: '', 9579: '', 12298: '', 12299: '', 25237: '', 31295: '', 22823: '', 12365: '', 19990: '', 10004: '', 1496: '', 39015: '', 36637: '', 20185: '', 25289: '', 21884: '', 31505: '', 23035: '', 32227: '', 31331: '', 9863: '', 9812: '', 13221: '', 128522: '', 9679: '', 10003: '', 151: '', 22235: '', 37326: '', 24744: '', 22909: '', 65292: '', 27489: '', 36814: '', 20358: '', 35370: '', 33521: '', 25991: '', 29256: '', 30340: '', 32173: '', 22522: '', 30334: '', 31185: '', 38614: '', 28982: '', 25105: '', 20497: '', 38568: '', 26178: '', 20320: '', 22312: '', 32232: '', 36655: '', 20294: '', 36889: '', 27425: '', 22519: '', 31558: '', 25152: '', 20351: '', 29992: '', 20006: '', 26410: '', 36948: '', 21040: '', 38656: '', 27714: '', 28310: '', 33509: '', 19981: '', 20171: '', 24847: '', 21487: '', 20197: '', 20013: '', 24863: '', 35613: '', 21443: '', 21152: '', 9773: '', 65381: '', 8704: '', 2346: '', 2347: '', 2364: '', 2327: '', 2332: '', 10149: '', 73843: '', 1499: '', 32933: '', 34217: '', 32218: '', 30333: '', 30707: '', 39365: '', 20184: '', 36817: '', 29699: '', 30952: '', 12288: '', 29066: '', 30476: '', 33894: '', 21271: '', 37089: '', 33446: '', 30010: '', 65295: '', 26449: '', 40643: '', 24062: '', 26082: '', 25925: '', 25913: '', 24180: '', 28858: '', 65377: '', 9685: '', 8255: '', 33775: '', 27665: '', 22283: '', 20301: '', 26044: '', 26481: '', 20126: '', 20849: '', 21644: '', 27954: '', 29694: '', 26377: '', 26368: '', 26089: '', 23526: '', 34892: '', 31435: '', 25010: '', 21046: '', 25104: '', 26376: '', 21069: '', 34987: '', 24291: '', 27867: '', 25215: '', 35469: '', 20195: '', 34920: '', 27880: '', 20170: '', 22240: '', 35201: '', 22303: '', 32622: '', 25110: '', 25919: '', 27835: '', 32032: '', 32780: '', 36890: '', 31281: '', 33274: '', 28771: '', 38450: '', 37096: '', 32879: '', 21512: '', 32654: '', 21209: '', 38498: '', 29544: '', 39636: '', 27833: '', 36664: '', 20986: '', 32068: '', 32340: '', 8242: '', 2350: '', 2335: '', 8485: '', 9763: '', 1769: '', 1758: '', 8060: '', 1352: '', 1407: '', 1740: '', 19968: '', 37070: '', 9821: '', 9818: '', 20839: '', 22320: '', 21313: '', 20843: '', 30465: '', 21371: '', 21482: '', 25351: '', 20659: '', 32113: '', 33539: '', 22285: '', 21363: '', 21253: '', 25324: '', 33945: '', 21476: '', 12289: '', 35199: '', 34255: '', 38738: '', 28023: '', 26032: '', 30086: '', 21488: '', 8196: '', 771: '', 9755: '', 9669: '', 10132: '', 128514: '', 49692: '', 54792: '', 51452: '', 51032: '', 32020: '', 34880: '', 32681: '', 23560: '', 26222: '', 23416: '', 32887: '', 29677: '', 12514: '', 12523: '', 12483: '', 12488: '', 12354: '', 12426: '', 35874: '', 7424: '', 22826: '', 28246: '', 40644: '', 31395: '', 22581: '', 22369: '', 24196: '', 34676: '', 34678: '', 4326: '', 38957: '', 21490: '', 35760: '', 22995: '', 7461: '', 8205: '', 2315: '', 680: '', 9992: '', 128516: '', 10152: '', 172: '', 10154: '', 699: '', 762: '', 12512: '', 815: '', 1250: '', 247: '', 7767: '', 128172: '', 2343: '', 9760: '', 8545: '', 12501: '', 12449: '', 12477: '', 12462: '', 12519: '', 7841: '', 8480: '', 8729: '', 65306: '', 175: '', 35498: '', 21966: '', 9614: '', 170: '', 10047: '', 12522: '', 12460: '', 12521: '', 12459: '', 52395: '', 45576: '', 21021: '', 38634: '', 27704: '', 8709: '', 12484: '', 8209: '', 2379: '', 27224: '', 26708: '', 20102: '', 22806: '', 29983: '', 65293: '', 181: '', 185: '', 3675: '', 705: '', 12296: '', 8970: '', 12297: '', 9752: '', 20581: '', 8222: '', 26716: '', 34349: '', 39640: '', 26657: '', 12507: '', 12473: '', 12463: '', 12502: '', 1509: '', 2958: '', 2972: '', 2950: '', 2965: '', 2994: '', 3016: '', 2993: '', 2970: '', 2997: '', 7919: '', 8805: '', 24456: '', 61623: '', 66356: '', 66361: '', 66354: '', 66352: '', 66367: '', 639: '', 8008: '', 8058: '', 8000: '', 8056: '', 8125: '', 8004: '', 8005: '', 7988: '', 8150: '', 8048: '', 7985: '', 7936: '', 7968: '', 8050: '', 8183: '', 7956: '', 8756: '', 35352: '', 37682: '', 12513: '', 29420: '', 26029: '', 20559: '', 12391: '', 36984: '', 53952: '', 48372: '', 54840: '', 47928: '', 49436: '', 54200: '', 51665: '', 50836: '', 52397: '', 12527: '', 12471: '', 9784: '', 23628: '', 32769: '', 27597: '', 11431: '', 11423: '', 11433: '', 11441: '', 35342: '', 35542: '', 2384: '', 1617: '', 8599: '', 8249: '', 8250: '', 26126: '', 28330: '', 65281: '', 11363: '', 7711: '', 7739: '', 21335: '', 8253: '', 7749: '', 30028: '', 9496: '', 9484: '', 128222: '', 128231: '', 38913: '', 38754: '', 20570: '', 21861: '', 20415: '', 33258: '', 24049: '', 35611: '', 29245: '', 23601: '', 12610: '', 22793: '', 26684: '', 35328: '', 29305: '', 21029: '', 27963: '', 8231: '', 29483: '', 3605: '', 3656: '', 3629: '', 3618: '', 3617: '', 3623: '', 2616: '', 2625: '', 2582: '', 713: '', 9574: '', 9577: '', 2313: '', 2354: '', 2333: '', 2370: '', 2305: '', 1181: '', 700: '', 7897: '', 7787: '', 26954: '', 23041: '', 21033: '', 37324: '', 9702: '', 9632: '', 12304: '', 36975: '', 21323: '', 12305: '', 24555: '', 20048: '', 10014: '', 8243: '', 52380: '', 47532: '', 47560: '', 44400: '', 21315: '', 39340: '', 44053: '', 47329: '', 23713: '', 45824: '', 50504: '', 23433: '', 44396: '', 24535: '', 21367: '', 21525: '', 24067: '', 24352: '', 37000: '', 38472: '', 30331: '', 33255: '', 27946: '', 28450: '', 26360: '', 21129: '', 28937: '', 34945: '', 34899: '', 21015: '', 496: '', 703: '', 35610: '', 25935: '', 23403: '', 28168: '', 29579: '', 20063: '', 20057: '', 32153: '', 37117: '', 24917: '', 27494: '', 23527: '', 704: '', 2407: '', 2408: '', 2409: '', 2410: '', 2411: '', 2412: '', 2413: '', 2414: '', 2415: '', 12431: '', 12383: '', 12388: '', 12387: '', 12371: '', 12393: '', 12429: '', 12509: '', 12486: '', 12451: '', 12454: '', 12467: '', 12450: '', 12372: '', 12475: '', 24525: '', 2358: '', 31935: '', 32005: '', 40860: '', 9643: '', 1729: '', 61607: '', 9745: '', 2949: '', 2980: '', 2995: '', 2975: '', 3015: '', 2979: '', 3006: '', 2985: '', 2984: '', 3008: '', 3019: '', 2962: '', 2953: '', 2969: '', 2951: '', 3846: '', 24433: '', 32773: '', 9474: '', 8210: '', 127876: '', 1705: '', 128521: '', 1199: '', 28059: '', 28644: '', 23567: '', 20992: '', 65077: '', 32043: '', 30722: '', 27877: '', 26417: '', 32526: '', 51312: '', 49440: '', 51064: '', 48124: '', 44277: '', 23545: '', 35805: '', 39029: '', 10086: '', 10070: '', 40881: '', 10007: '', 12385: '', 40599: '', 32291: '', 9757: '', 10027: '', 7853: '', 173: '', 7885: '', 7909: '', 7865: '', 7534: '', 8286: '', 20685: '', 21214: '', 21205: '', 21172: '', 48736: '', 460: '', 9731: '', 22269: '', 24066: '', 12414: '', 65297: '', 65296: '', 12541: '', 12526: '', 65417: '', 8580: '', 39033: '', 30446: '', 32461: '', 7879: '', 7917: '', 432: '', 417: '', 8234: '', 8236: '', 9876: '', 30418: '', 841: '', 839: '', 798: '', 854: '', 828: '', 813: '', 851: '', 849: '', 776: '', 777: '', 535: '', 825: '', 818: '', 872: '', 878: '', 770: '', 772: '', 7769: '', 811: '', 805: '', 858: '', 796: '', 857: '', 845: '', 816: '', 826: '', 809: '', 791: '', 787: '', 871: '', 873: '', 831: '', 868: '', 782: '', 774: '', 537: '', 177: '', 157: '', 45216: '', 51664: '', 49849: '', 52252: '', 44600: '', 51105: '', 51060: '', 51088: '', 48708: '', 9996: '', 9642: '', 8322: '', 38886: '', 26234: '', 10008: '', 9500: '', 128546: '', 128515: '', 52377: '', 45684: '', 45336: '', 7901: '', 7915: '', 8669: '', 1620: '', 10020: '', 23383: '', 12452: '', 12461: '', 38988: '', 38306: '', 20418: '', 12434: '', 12369: '', 26041: '', 33391: '', 24605: '', 21516: '', 12376: '', 20363: '', 12400: '', 22580: '', 12506: '', 20140: '', 24093: '', 12428: '', 20182: '', 21619: '', 37291: '', 12503: '', 12499: '', 12517: '', 12374: '', 8743: '', 23627: '', 19971: '', 22675: '', 477: '', 8498: '', 31062: '', 2311: '', 2374: '', 7877: '', 12455: '', 10077: '', 10078: '', 9995: '', 127996: '', 9790: '', 9789: '', 128527: '', 35363: '', 32066: '', 27490: '', 20307: '', 20206: '', 23450: '', 21629: '', 20196: '', 24050: '', 8712: '', 4334: '', 4314: '', 4308: '', 2439: '', 2468: '', 2495: '', 2489: '', 2488: '', 61501: '', 20808: '', 8747: '', 9668: '', 2588: '', 2620: '', 2603: '', 2608: '', 2600: '', 2622: '', 2606: '', 24635: '', 30563: '', 21103: '', 9675: '', 27931: '', 38525: '', 38263: '', 9662: '', 21943: '', 28779: '', 22120: '', 29467: '', 26588: '', 1472: '', 1662: '', 23569: '', 7871: '', 21453: '', 36870: '', 12510: '', 38360: '', 21531: '', 8597: '', 9758: '', 4324: '', 4318: '', 4317: '', 4321: '', 4309: '', 25842: '', 37329: '', 1351: '', 1399: '', 1387: '', 9756: '', 3482: '', 3540: '', 3515: '', 3507: '', 3520: '', 3501: '', 3530: '', 1891: '', 1784: '', 1879: '', 1787: '', 1875: '', 1712: '', 1692: '', 1708: '', 8616: '', 1790: '', 1887: '', 1884: '', 1901: '', 1772: '', 1781: '', 1889: '', 1717: '', 1670: '', 12525: '', 12481: '', 29105: '', 38622: '', 34507: '', 8239: '', 8195: '', 8829: '', 11383: '', 1157: '', 8828: '', 4051: '', 7712: '', 7783: '', 2341: '', 10226: '', 8134: '', 7953: '', 8118: '', 7960: '', 2453: '', 2478: '', 2509: '', 2477: '', 2463: '', 2507: '', 2480: '', 2479: '', 2492: '', 2503: '', 2460: '', 128077: '', 8467: '', 8477: '', 8461: '', 10057: '', 29356: '', 20882: '', 38522: '', 9650: '', 1572: '', 8474: '', 52832: '', 51648: '', 46020: '', 20304: '', 21161: '', 63283: '', 63286: '', 63280: '', 63282: '', 63285: '', 63284: '', 63281: '', 63289: '', 63198: '', 63287: '', 63288: '', 8308: '', 8304: '', 10002: '', 19978: '', 19996: '', 27721: '', 25163: '', 25615: '', 28639: '', 29031: '', 29281: '', 22777: '', 30091: '', 29277: '', 38463: '', 21513: '', 24107: '', 36002: '', 36060: '', 25776: '', 27663: '', 12491: '', 12470: '', 12504: '', 20329: '', 31069: '', 36807: '', 20010: '', 1127: '', 27827: '', 12397: '', 8002: '', 33261: '', 22920: '', 28866: '', 8644: '', 8721: '', 12489: '', 65302: '', 65304: '', 20844: '', 12495: '', 29289: '', 35486: '', 24773: '', 32025: '', 39080: '', 33337: '', 800: '', 20061: '', 20853: '', 34907: '', 12419: '', 12402: '', 12423: '', 24859: '', 23445: '', 9665: '', 8861: '', 8857: '', 26792: '', 37240: '', 38078: '', 1503: '', 1461: '', 820: '', 2376: '', 44256: '', 44592: '', 49548: '', 26609: '', 7488: '', 7468: '', 7480: '', 7479: '', 127908: '', 8188: '', 7491: '', 7503: '', 44060: '', 45716: '', 46412: '', 47140: '', 50500: '', 50556: '', 47579: '', 51080: '', 45796: '', 2351: '', 128532: '', 128156: '', 1827: '', 1816: '', 1834: '', 1821: '', 1808: '', 12498: '', 35069: '', 34220: '', 26985: '', 20689: '', 9711: '', 2508: '', 8207: '', 35519: '', 50976: '', 54732: '', 23432: '', 10809: '', 2536: '', 2534: '', 2535: '', 428: '', 22346: '', 38520: '', 36557: '', 23448: '', 2310: '', 64258: '', 28385: '', 28165: '', 20498: '', 36864: '', 35770: '', 37101: '', 21518: '', 26397: '', 39854: '', 65395: '', 65384: '', 65399: '', 65421: '', 65439: '', 65411: '', 65438: '', 65393: '', 25423: '', 36896: '', 23550: '', 25239: '', 8869: '', 10140: '', 128512: '', 1425: '', 28436: '', 1244: '', 22899: '', 31070: '', 12382: '', 32764: '', 23611: '', 1178: '', 1179: '', 8235: '', 8727: '', 63193: '', 7791: '', 21672: '', 509: '', 63228: '', 63233: '', 10029: '', 9398: '', 12959: '', 145: '', 146: '', 12458: '', 12487: '', 128584: '', 128585: '', 128586: '', 3716: '', 3771: '', 3749: '', 3751: '', 3768: '', 3784: '', 3745: '', 3760: '', 3719: '', 3735: '', 3757: '', 3761: '', 3764: '', 3732: '', 3765: '', 3720: '', 3722: '', 3780: '', 3733: '', 3755: '', 3772: '', 3769: '', 3747: '', 3778: '', 3804: '', 3758: '', 21344: '', 46021: '', 23798: '', 31481: '', 9684: '', 20108: '', 21382: '', 12532: '', 12456: '', 12494: '', 3065: '', 738: '', 48155: '', 52840: '', 12613: '', 12599: '', 23481: '', 2337: '', 2377: '', 2324: '', 9608: '', 37504: '', 32650: '', 33276: '', 20117: '', 20736: '', 38515: '', 32988: '', 21555: '', 8497: '', 36825: '', 20160: '', 20040: '', 30058: '', 25915: '', 25731: '', 1256: '', 1257: '', 21328: '', 7889: '', 8275: '', 22768: '', 25588: '', 7567: '', 990: '', 25490: '', 34398: '', 27798: '', 32260: '', 26093: '', 7969: '', 33713: '', 33436: '', 26223: '', 28548: '', 21306: '', 24220: '', 32593: '', 31449: '', 4343: '', 1711: '', 32705: '', 8208: '', 7863: '', 7851: '', 7887: '', 7855: '', 7845: '', 7881: '', 7843: '', 7875: '', 7907: '', 7857: '', 12418: '', 679: '', 29642: '', 29786: '', 24178: '', 36215: '', 9762: '', 8834: '', 9487: '', 9473: '', 9491: '', 9475: '', 9556: '', 9552: '', 9559: '', 9553: '', 9562: '', 9565: '', 9495: '', 9499: '', 38271: '', 39048: '', 40575: '', 383: '', 3232: '', 30410: '', 27874: '', 28734: '', 19975: '', 19976: '', 20253: '', 10216: '', 10217: '', 8746: '', 61514: '', 9654: '', 986: '', 359: '', 38313: '', 23458: '', 31925: '', 36234: '', 1672: '', 9664: '', 26753: '', 1161: '', 12497: '', 9651: '', 2330: '', 2406: '', 2326: '', 2348: '', 2371: '', 2336: '', 702: '', 9701: '', 128169: '', 44221: '', 49345: '', 1347: '', 5831: '', 5817: '', 5855: '', 7511: '', 6654: '', 22633: '', 35895: '', 26647: '', 26519: '', 24935: '', 28193: '', 36794: '', 20860: '', 12409: '', 25240: '', 24693: '', 988: '', 880: '', 1018: '', 1015: '', 882: '', 38587: '', 12465: '', 27770: '', 12417: '', 12479: '', 12469: '', 12370: '', 12496: '', 12492: '', 12421: '', 12403: '', 12405: '', 12508: '', 12380: '', 12468: '', 12490: '', 12505: '', 12408: '', 27818: '', 28396: '', 21830: '', 21697: '', 21517: '', 9764: '', 8317: '', 8318: '', 12500: ''}\n"
          ]
        }
      ],
      "source": [
        "# Combine standard punctuation and non-Latin characters\n",
        "bad_chars = string.punctuation + \"\".join(list(over_127.symbol))\n",
        "print(\"Non-Latin characters:\", bad_chars)\n",
        "\n",
        "# Create a translation table to remove those characters\n",
        "translation_table = str.maketrans(dict.fromkeys(bad_chars, \"\"))\n",
        "print(translation_table)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCbO0HrngM46"
      },
      "source": [
        "We can see that the tweets are written in different languages, and not all of them are European, where we can determine sentiment based on context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CdT7FFvgM47",
        "outputId": "17130c35-e6e9-4e98-acd1-f3bb2a20644c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total length of the original text: 62209334\n"
          ]
        }
      ],
      "source": [
        "# Calculate the length of the original (uncleaned) text\n",
        "data['dirty_len'] = data.text.apply(len)\n",
        "data.head()\n",
        "\n",
        "print(\"Total length of the original text:\", data.dirty_len.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPIpJ-b5gM47",
        "outputId": "b5185bef-bce5-4061-bd4f-809af5d16d3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning at 2025-04-06 20:11:48.408266\n",
            "End cleaning at 2025-04-06 20:11:51.773817\n"
          ]
        }
      ],
      "source": [
        "# Function to remove unwanted characters using the translation table\n",
        "def translate_text(text):\n",
        "    return text.translate(translation_table)\n",
        "\n",
        "print(f\"Start cleaning at {datetime.now()}\")\n",
        "data['text'] = data.text.apply(translate_text)\n",
        "print(f\"End cleaning at {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZGf2SZrgM47",
        "outputId": "cb15cb2e-c4a2-4e13-d4b9-6b55e9f65634"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text after removing non-Latin characters: 59622652\n",
            "Characters removed: 2586682\n"
          ]
        }
      ],
      "source": [
        "# Calculate length of text after removing non-Latin characters\n",
        "data['clean_len'] = data.text.apply(len)\n",
        "data.head()\n",
        "\n",
        "print(\"Length of text after removing non-Latin characters:\", data.clean_len.sum())\n",
        "print(\"Characters removed:\", data.dirty_len.sum() - data.clean_len.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDxY8d_YgM47",
        "outputId": "b802a3b2-7aff-44f1-dc0f-5bbbde560b6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extra whitespaces: 36766\n",
            "Total rows: 159292\n"
          ]
        }
      ],
      "source": [
        "# Function to check for multiple consecutive whitespaces\n",
        "def check_multiple_whitespaces(text):\n",
        "    return re.search(r'\\s{2,}', text) is not None\n",
        "\n",
        "print(\"Extra whitespaces:\", data.text.apply(check_multiple_whitespaces).sum())\n",
        "print(\"Total rows:\", len(data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ntcSmGbgM47",
        "outputId": "076b1e96-7007-4ebc-d37a-762d4e9a4782"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning at 2025-04-06 20:12:55.826014\n",
            "End cleaning at 2025-04-06 20:12:57.037746\n",
            "Extra whitespaces remaining: 0\n"
          ]
        }
      ],
      "source": [
        "# Function to remove multiple consecutive whitespaces\n",
        "def remove_multiple_whitespaces(text):\n",
        "    return re.sub(r'\\s{2,}', ' ', text)\n",
        "\n",
        "print(f\"Start cleaning at {datetime.now()}\")\n",
        "data['text'] = data.text.apply(remove_multiple_whitespaces)\n",
        "print(f\"End cleaning at {datetime.now()}\")\n",
        "\n",
        "print(\"Extra whitespaces remaining:\", data.text.apply(check_multiple_whitespaces).sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIP_7_dngM47",
        "outputId": "1c24276e-2b3a-485e-ef09-9c38686af994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  \\\n",
            "0           0   \n",
            "1           1   \n",
            "2           2   \n",
            "3           3   \n",
            "4           4   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                        Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                   Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info   \n",
            "3   More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport    \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You sir are my hero Any chance you remember what page thats on   \n",
            "\n",
            "   toxic  dirty_len  clean_len  \n",
            "0      0        264        254  \n",
            "1      0        111         99  \n",
            "2      0        233        227  \n",
            "3      0        619        598  \n",
            "4      0         67         62  \n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSzCoHl9gM48"
      },
      "outputs": [],
      "source": [
        "def split_text(text):\n",
        "    return text.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "TRtXHZr5gM48",
        "outputId": "97fad044-6c25-411e-c4e9-c42245fb6a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning at 2025-04-06 20:14:37.312482\n",
            "End cleaning at 2025-04-06 20:14:38.641124\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Unnamed: 0     int64\n",
              "text          object\n",
              "toxic          int64\n",
              "dirty_len      int64\n",
              "clean_len      int64\n",
              "words         object\n",
              "dtype: object"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"Start cleaning at {datetime.now()}\")\n",
        "data['words'] = data.text.apply(split_text)\n",
        "print(f\"End cleaning at {datetime.now()}\")\n",
        "\n",
        "# Check data types\n",
        "data.dtypes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1cWy6-7gM5A",
        "outputId": "91cb955d-2f6f-4fa2-8a5d-fc6062068c23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  \\\n",
            "0           0   \n",
            "1           1   \n",
            "2           2   \n",
            "3           3   \n",
            "4           4   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                        Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                   Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info   \n",
            "3   More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport    \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You sir are my hero Any chance you remember what page thats on   \n",
            "\n",
            "   toxic  dirty_len  clean_len  \\\n",
            "0      0        264        254   \n",
            "1      0        111         99   \n",
            "2      0        233        227   \n",
            "3      0        619        598   \n",
            "4      0         67         62   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           words  \n",
            "0                                                                                                                                                                                                                                                                                                                                     [Explanation, Why, the, edits, made, under, my, username, Hardcore, Metallica, Fan, were, reverted, They, werent, vandalisms, just, closure, on, some, GAs, after, I, voted, at, New, York, Dolls, FAC, And, please, dont, remove, the, template, from, the, talk, page, since, Im, retired, now892053827]  \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Daww, He, matches, this, background, colour, Im, seemingly, stuck, with, Thanks, talk, 2151, January, 11, 2016, UTC]  \n",
            "2                                                                                                                                                                                                                                                                                                                                                                 [Hey, man, Im, really, not, trying, to, edit, war, Its, just, that, this, guy, is, constantly, removing, relevant, information, and, talking, to, me, through, edits, instead, of, my, talk, page, He, seems, to, care, more, about, the, formatting, than, the, actual, info]  \n",
            "3  [More, I, cant, make, any, real, suggestions, on, improvement, I, wondered, if, the, section, statistics, should, be, later, on, or, a, subsection, of, types, of, accidents, I, think, the, references, may, need, tidying, so, that, they, are, all, in, the, exact, same, format, ie, date, format, etc, I, can, do, that, later, on, if, noone, else, does, first, if, you, have, any, preferences, for, formatting, style, on, references, or, want, to, do, it, yourself, please, let, me, know, There, appears, to, be, a, backlog, on, articles, for, review, so, I, guess, there, may, be, a, delay, until, a, reviewer, turns, ...]  \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [You, sir, are, my, hero, Any, chance, you, remember, what, page, thats, on]  \n"
          ]
        }
      ],
      "source": [
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "A_pbSC-bgM5A",
        "outputId": "213ef3de-95c1-42e5-cf1c-f60821ab46b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  \\\n",
            "0           0   \n",
            "1           1   \n",
            "2           2   \n",
            "3           3   \n",
            "4           4   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                        Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                   Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info   \n",
            "3   More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport    \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You sir are my hero Any chance you remember what page thats on   \n",
            "\n",
            "   toxic  dirty_len  clean_len  \\\n",
            "0      0        264        254   \n",
            "1      0        111         99   \n",
            "2      0        233        227   \n",
            "3      0        619        598   \n",
            "4      0         67         62   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           words  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                     [Explanation, Why, the, edits, made, under, my, username, Hardcore, Metallica, Fan, were, reverted, They, werent, vandalisms, just, closure, on, some, GAs, after, I, voted, at, New, York, Dolls, FAC, And, please, dont, remove, the, template, from, the, talk, page, since, Im, retired, now892053827]   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [Daww, He, matches, this, background, colour, Im, seemingly, stuck, with, Thanks, talk, 2151, January, 11, 2016, UTC]   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                 [Hey, man, Im, really, not, trying, to, edit, war, Its, just, that, this, guy, is, constantly, removing, relevant, information, and, talking, to, me, through, edits, instead, of, my, talk, page, He, seems, to, care, more, about, the, formatting, than, the, actual, info]   \n",
            "3  [More, I, cant, make, any, real, suggestions, on, improvement, I, wondered, if, the, section, statistics, should, be, later, on, or, a, subsection, of, types, of, accidents, I, think, the, references, may, need, tidying, so, that, they, are, all, in, the, exact, same, format, ie, date, format, etc, I, can, do, that, later, on, if, noone, else, does, first, if, you, have, any, preferences, for, formatting, style, on, references, or, want, to, do, it, yourself, please, let, me, know, There, appears, to, be, a, backlog, on, articles, for, review, so, I, guess, there, may, be, a, delay, until, a, reviewer, turns, ...]   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [You, sir, are, my, hero, Any, chance, you, remember, what, page, thats, on]   \n",
            "\n",
            "   word_count  \n",
            "0          43  \n",
            "1          17  \n",
            "2          42  \n",
            "3         109  \n",
            "4          13  \n"
          ]
        }
      ],
      "source": [
        "data['word_count'] = data.words.apply(len)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "id": "jl_eebEfgM5B",
        "outputId": "1c3e8765-41cc-4866-e141-141365c8c495"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start cleaning at 2025-04-06 20:14:51.644360\n",
            "End cleaning at 2025-04-06 20:14:53.677783\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>dirty_len</th>\n",
              "      <th>clean_len</th>\n",
              "      <th>words</th>\n",
              "      <th>word_count</th>\n",
              "      <th>word_count_cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827</td>\n",
              "      <td>0</td>\n",
              "      <td>264</td>\n",
              "      <td>254</td>\n",
              "      <td>[Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]</td>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>99</td>\n",
              "      <td>[Daww, He, matches, background, colour, Im, seemingly, stuck, Thanks, talk, 2151, January, 11, 2016, UTC]</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info</td>\n",
              "      <td>0</td>\n",
              "      <td>233</td>\n",
              "      <td>227</td>\n",
              "      <td>[Hey, man, Im, really, trying, edit, war, Its, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, He, seems, care, formatting, actual, info]</td>\n",
              "      <td>42</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport</td>\n",
              "      <td>0</td>\n",
              "      <td>619</td>\n",
              "      <td>598</td>\n",
              "      <td>[More, I, cant, make, real, suggestions, improvement, I, wondered, section, statistics, later, subsection, types, accidents, I, think, references, may, need, tidying, exact, format, ie, date, format, etc, I, later, noone, else, first, preferences, formatting, style, references, want, please, let, know, There, appears, backlog, articles, review, I, guess, may, delay, reviewer, turns, Its, listed, relevant, form, eg, WikipediaGoodarticlenominationsTransport]</td>\n",
              "      <td>109</td>\n",
              "      <td>57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>You sir are my hero Any chance you remember what page thats on</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>62</td>\n",
              "      <td>[You, sir, hero, Any, chance, remember, page, thats]</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                        Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                   Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info   \n",
              "3   More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport    \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You sir are my hero Any chance you remember what page thats on   \n",
              "\n",
              "   toxic  dirty_len  clean_len  \\\n",
              "0      0        264        254   \n",
              "1      0        111         99   \n",
              "2      0        233        227   \n",
              "3      0        619        598   \n",
              "4      0         67         62   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                          words  \\\n",
              "0                                                                                                                                                                                                                                     [Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                     [Daww, He, matches, background, colour, Im, seemingly, stuck, Thanks, talk, 2151, January, 11, 2016, UTC]   \n",
              "2                                                                                                                                                                                                                                                                                              [Hey, man, Im, really, trying, edit, war, Its, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, He, seems, care, formatting, actual, info]   \n",
              "3  [More, I, cant, make, real, suggestions, improvement, I, wondered, section, statistics, later, subsection, types, accidents, I, think, references, may, need, tidying, exact, format, ie, date, format, etc, I, later, noone, else, first, preferences, formatting, style, references, want, please, let, know, There, appears, backlog, articles, review, I, guess, may, delay, reviewer, turns, Its, listed, relevant, form, eg, WikipediaGoodarticlenominationsTransport]   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                          [You, sir, hero, Any, chance, remember, page, thats]   \n",
              "\n",
              "   word_count  word_count_cleaned  \n",
              "0          43                  31  \n",
              "1          17                  15  \n",
              "2          42                  24  \n",
              "3         109                  57  \n",
              "4          13                   8  "
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stop_words(words):\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "print(f\"Start cleaning at {datetime.now()}\")\n",
        "data['words'] = data.words.apply(remove_stop_words)\n",
        "print(f\"End cleaning at {datetime.now()}\")\n",
        "\n",
        "data['word_count_cleaned'] = data.words.apply(len)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8QLOKs8gM5B",
        "outputId": "07f368f2-6f54-4496-d50b-8ed41b756784"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Words removed: 4389329\n",
            "Words remaining: 6136150\n"
          ]
        }
      ],
      "source": [
        "print(\"Words removed:\", data['word_count'].sum() - data['word_count_cleaned'].sum())\n",
        "print(\"Words remaining:\", data['word_count_cleaned'].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX6anWDtqDcy"
      },
      "outputs": [],
      "source": [
        "nltk.data.path.append('/Users/eugenia/nltk_data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8L0gEPYqDcy",
        "outputId": "2532bb7f-6aea-4176-c684-87fd67f39af6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger', download_dir='/Users/eugenia/nltk_data')\n",
        "nltk.download('wordnet', download_dir='/Users/eugenia/nltk_data')\n",
        "nltk.download('omw-1.4', download_dir='/Users/eugenia/nltk_data')\n",
        "nltk.download('punkt', download_dir='/Users/eugenia/nltk_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJX5euHRqDcy",
        "outputId": "e652d790-8fb0-4717-9d26-0b2ceb08c63a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /Users/eugenia/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /Users/eugenia/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /Users/eugenia/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger_eng', download_dir='/Users/eugenia/nltk_data', force=True)\n",
        "nltk.download('wordnet', download_dir='/Users/eugenia/nltk_data', force=True)\n",
        "nltk.download('omw-1.4', download_dir='/Users/eugenia/nltk_data', force=True)\n",
        "nltk.download('punkt', download_dir='/Users/eugenia/nltk_data', force=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hsKljgpRqDcy",
        "outputId": "1c6fb469-8d15-47d3-a749-39395ab66156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/Users/eugenia/nltk_data', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/nltk_data', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/share/nltk_data', '/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/Users/eugenia/nltk_data']\n",
            "/Users/eugenia/nltk_data/taggers/averaged_perceptron_tagger\n"
          ]
        }
      ],
      "source": [
        "print(nltk.data.path)\n",
        "print(nltk.find('taggers/averaged_perceptron_tagger'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        },
        "id": "W5qgUsJ3gM5B",
        "outputId": "16e17b54-6f60-40af-8411-93afb6da1c71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Start preprocessing at 2025-04-06 20:17:46.444512\n",
            "End preprocessing at 2025-04-06 20:22:49.628601\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>dirty_len</th>\n",
              "      <th>clean_len</th>\n",
              "      <th>words</th>\n",
              "      <th>word_count</th>\n",
              "      <th>word_count_cleaned</th>\n",
              "      <th>words_lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827</td>\n",
              "      <td>0</td>\n",
              "      <td>264</td>\n",
              "      <td>254</td>\n",
              "      <td>[Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]</td>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>[explanation, why, edits, make, username, hardcore, metallica, fan, revert, they, werent, vandalism, closure, gas, i, vote, new, york, doll, fac, and, please, dont, remove, template, talk, page, since, im, retire]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC</td>\n",
              "      <td>0</td>\n",
              "      <td>111</td>\n",
              "      <td>99</td>\n",
              "      <td>[Daww, He, matches, background, colour, Im, seemingly, stuck, Thanks, talk, 2151, January, 11, 2016, UTC]</td>\n",
              "      <td>17</td>\n",
              "      <td>15</td>\n",
              "      <td>[daww, he, match, background, colour, im, seemingly, stuck, thanks, talk, january, utc]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info</td>\n",
              "      <td>0</td>\n",
              "      <td>233</td>\n",
              "      <td>227</td>\n",
              "      <td>[Hey, man, Im, really, trying, edit, war, Its, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, He, seems, care, formatting, actual, info]</td>\n",
              "      <td>42</td>\n",
              "      <td>24</td>\n",
              "      <td>[hey, man, im, really, try, edit, war, it, guy, constantly, remove, relevant, information, talk, edits, instead, talk, page, he, seem, care, format, actual, info]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport</td>\n",
              "      <td>0</td>\n",
              "      <td>619</td>\n",
              "      <td>598</td>\n",
              "      <td>[More, I, cant, make, real, suggestions, improvement, I, wondered, section, statistics, later, subsection, types, accidents, I, think, references, may, need, tidying, exact, format, ie, date, format, etc, I, later, noone, else, first, preferences, formatting, style, references, want, please, let, know, There, appears, backlog, articles, review, I, guess, may, delay, reviewer, turns, Its, listed, relevant, form, eg, WikipediaGoodarticlenominationsTransport]</td>\n",
              "      <td>109</td>\n",
              "      <td>57</td>\n",
              "      <td>[more, i, cant, make, real, suggestion, improvement, i, wonder, section, statistic, later, subsection, type, accident, i, think, reference, may, need, tidy, exact, format, ie, date, format, etc, i, later, noone, else, first, preference, format, style, reference, want, please, let, know, there, appear, backlog, article, review, i, guess, may, delay, reviewer, turn, it, list, relevant, form, eg, wikipediagoodarticlenominationstransport]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>You sir are my hero Any chance you remember what page thats on</td>\n",
              "      <td>0</td>\n",
              "      <td>67</td>\n",
              "      <td>62</td>\n",
              "      <td>[You, sir, hero, Any, chance, remember, page, thats]</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>[you, sir, hero, any, chance, remember, page, thats]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "1           1   \n",
              "2           2   \n",
              "3           3   \n",
              "4           4   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
              "0                                                                                                                                                                                                                                                                                                                                                        Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC   \n",
              "2                                                                                                                                                                                                                                                                                                                                                                                   Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info   \n",
              "3   More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport    \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You sir are my hero Any chance you remember what page thats on   \n",
              "\n",
              "   toxic  dirty_len  clean_len  \\\n",
              "0      0        264        254   \n",
              "1      0        111         99   \n",
              "2      0        233        227   \n",
              "3      0        619        598   \n",
              "4      0         67         62   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                          words  \\\n",
              "0                                                                                                                                                                                                                                     [Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]   \n",
              "1                                                                                                                                                                                                                                                                                                                                                                     [Daww, He, matches, background, colour, Im, seemingly, stuck, Thanks, talk, 2151, January, 11, 2016, UTC]   \n",
              "2                                                                                                                                                                                                                                                                                              [Hey, man, Im, really, trying, edit, war, Its, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, He, seems, care, formatting, actual, info]   \n",
              "3  [More, I, cant, make, real, suggestions, improvement, I, wondered, section, statistics, later, subsection, types, accidents, I, think, references, may, need, tidying, exact, format, ie, date, format, etc, I, later, noone, else, first, preferences, formatting, style, references, want, please, let, know, There, appears, backlog, articles, review, I, guess, may, delay, reviewer, turns, Its, listed, relevant, form, eg, WikipediaGoodarticlenominationsTransport]   \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                                          [You, sir, hero, Any, chance, remember, page, thats]   \n",
              "\n",
              "   word_count  word_count_cleaned  \\\n",
              "0          43                  31   \n",
              "1          17                  15   \n",
              "2          42                  24   \n",
              "3         109                  57   \n",
              "4          13                   8   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                         words_lemmatized  \n",
              "0                                                                                                                                                                                                                                   [explanation, why, edits, make, username, hardcore, metallica, fan, revert, they, werent, vandalism, closure, gas, i, vote, new, york, doll, fac, and, please, dont, remove, template, talk, page, since, im, retire]  \n",
              "1                                                                                                                                                                                                                                                                                                                                                                 [daww, he, match, background, colour, im, seemingly, stuck, thanks, talk, january, utc]  \n",
              "2                                                                                                                                                                                                                                                                                      [hey, man, im, really, try, edit, war, it, guy, constantly, remove, relevant, information, talk, edits, instead, talk, page, he, seem, care, format, actual, info]  \n",
              "3  [more, i, cant, make, real, suggestion, improvement, i, wonder, section, statistic, later, subsection, type, accident, i, think, reference, may, need, tidy, exact, format, ie, date, format, etc, i, later, noone, else, first, preference, format, style, reference, want, please, let, know, there, appear, backlog, article, review, i, guess, may, delay, reviewer, turn, it, list, relevant, form, eg, wikipediagoodarticlenominationstransport]  \n",
              "4                                                                                                                                                                                                                                                                                                                                                                                                    [you, sir, hero, any, chance, remember, page, thats]  "
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "\n",
        "def lemmatize_word_list(word_list):\n",
        "    return [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in word_list]\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, list):\n",
        "\n",
        "        words = [word.lower() for word in text if word.isalpha()]\n",
        "    else:\n",
        "\n",
        "        expanded_text = contractions.fix(text)\n",
        "        words = nltk.word_tokenize(expanded_text)\n",
        "        words = [word.lower() for word in words if word.isalpha()]\n",
        "\n",
        "\n",
        "    lemmatized_words = lemmatize_word_list(words)\n",
        "\n",
        "    return lemmatized_words\n",
        "\n",
        "print(f\"Start preprocessing at {datetime.now()}\")\n",
        "\n",
        "data['words_lemmatized'] = data['words'].apply(preprocess_text)\n",
        "print(f\"End preprocessing at {datetime.now()}\")\n",
        "\n",
        "\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WM5KDqJgM5C",
        "outputId": "c1e732df-3f24-4ea9-8a3e-c7aa3f1be6d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words: 292530\n",
            "Unique lemmas: 197156\n"
          ]
        }
      ],
      "source": [
        "# Flatten the list of tokenized words and count unique ones\n",
        "all_words = list(itertools.chain.from_iterable(data.words))\n",
        "print(\"Unique words:\", len(set(all_words)))\n",
        "\n",
        "# Flatten the list of lemmatized words and count unique lemmas\n",
        "all_words_lemmatized = list(itertools.chain.from_iterable(data.words_lemmatized))\n",
        "print(\"Unique lemmas:\", len(set(all_words_lemmatized)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "id": "NFDhVvH3gM5C",
        "outputId": "05313656-3262-444b-e312-5e4359faf9e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Unnamed: 0  \\\n",
            "0           0   \n",
            "1           1   \n",
            "2           2   \n",
            "3           3   \n",
            "4           4   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   text  \\\n",
            "0                                                                                                                                                                                                                                                                                                                                                        Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC   \n",
            "2                                                                                                                                                                                                                                                                                                                                                                                   Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info   \n",
            "3   More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport    \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You sir are my hero Any chance you remember what page thats on   \n",
            "\n",
            "   toxic  dirty_len  clean_len  \\\n",
            "0      0        264        254   \n",
            "1      0        111         99   \n",
            "2      0        233        227   \n",
            "3      0        619        598   \n",
            "4      0         67         62   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                          words  \\\n",
            "0                                                                                                                                                                                                                                     [Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                     [Daww, He, matches, background, colour, Im, seemingly, stuck, Thanks, talk, 2151, January, 11, 2016, UTC]   \n",
            "2                                                                                                                                                                                                                                                                                              [Hey, man, Im, really, trying, edit, war, Its, guy, constantly, removing, relevant, information, talking, edits, instead, talk, page, He, seems, care, formatting, actual, info]   \n",
            "3  [More, I, cant, make, real, suggestions, improvement, I, wondered, section, statistics, later, subsection, types, accidents, I, think, references, may, need, tidying, exact, format, ie, date, format, etc, I, later, noone, else, first, preferences, formatting, style, references, want, please, let, know, There, appears, backlog, articles, review, I, guess, may, delay, reviewer, turns, Its, listed, relevant, form, eg, WikipediaGoodarticlenominationsTransport]   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                          [You, sir, hero, Any, chance, remember, page, thats]   \n",
            "\n",
            "   word_count  word_count_cleaned  \\\n",
            "0          43                  31   \n",
            "1          17                  15   \n",
            "2          42                  24   \n",
            "3         109                  57   \n",
            "4          13                   8   \n",
            "\n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                         words_lemmatized  \\\n",
            "0                                                                                                                                                                                                                                   [explanation, why, edits, make, username, hardcore, metallica, fan, revert, they, werent, vandalism, closure, gas, i, vote, new, york, doll, fac, and, please, dont, remove, template, talk, page, since, im, retire]   \n",
            "1                                                                                                                                                                                                                                                                                                                                                                 [daww, he, match, background, colour, im, seemingly, stuck, thanks, talk, january, utc]   \n",
            "2                                                                                                                                                                                                                                                                                      [hey, man, im, really, try, edit, war, it, guy, constantly, remove, relevant, information, talk, edits, instead, talk, page, he, seem, care, format, actual, info]   \n",
            "3  [more, i, cant, make, real, suggestion, improvement, i, wonder, section, statistic, later, subsection, type, accident, i, think, reference, may, need, tidy, exact, format, ie, date, format, etc, i, later, noone, else, first, preference, format, style, reference, want, please, let, know, there, appear, backlog, article, review, i, guess, may, delay, reviewer, turn, it, list, relevant, form, eg, wikipediagoodarticlenominationstransport]   \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                    [you, sir, hero, any, chance, remember, page, thats]   \n",
            "\n",
            "   word_count_cleaned_no_digits  \n",
            "0                            30  \n",
            "1                            12  \n",
            "2                            24  \n",
            "3                            57  \n",
            "4                             8  \n"
          ]
        }
      ],
      "source": [
        "def remove_digits(words):\n",
        "\n",
        "    return [word for word in words if not word.isdigit()]\n",
        "\n",
        "data.words_lemmatized = data.words_lemmatized.apply(remove_digits)\n",
        "data[\"word_count_cleaned_no_digits\"] = data.words_lemmatized.apply(len)\n",
        "print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apjCDCjugM5C",
        "outputId": "cab0f39a-d3b5-41ef-a642-65d287103dbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets: 159292\n",
            "Words containing digits removed: 149272\n",
            "Words remaining: 5986878\n"
          ]
        }
      ],
      "source": [
        "# Function to remove words that contain any digits\n",
        "def remove_words_with_digits(words):\n",
        "    return [word for word in words if not any(char.isdigit() for char in word)]\n",
        "\n",
        "# Apply the function to the lemmatized words\n",
        "data.words_lemmatized = data.words_lemmatized.apply(remove_words_with_digits)\n",
        "data[\"word_count_cleaned_no_digits\"] = data.words_lemmatized.apply(len)\n",
        "\n",
        "print(\"Number of tweets:\", data.shape[0])\n",
        "print(\"Words containing digits removed:\", data.word_count_cleaned.sum() - data.word_count_cleaned_no_digits.sum())\n",
        "print(\"Words remaining:\", data.word_count_cleaned_no_digits.sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1q7rESVgM5C",
        "outputId": "cec8cf05-59c7-4282-87e1-82113fcd05a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique words: 197130\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[('explanation', 55141),\n",
              " ('why', 187243),\n",
              " ('edits', 49530),\n",
              " ('make', 100610),\n",
              " ('username', 179991),\n",
              " ('hardcore', 69984),\n",
              " ('metallica', 105325),\n",
              " ('fan', 56348),\n",
              " ('revert', 141591),\n",
              " ('they', 169475),\n",
              " ('werent', 186396),\n",
              " ('vandalism', 181636),\n",
              " ('closure', 30992),\n",
              " ('gas', 63330),\n",
              " ('vote', 184092),\n",
              " ('new', 113804),\n",
              " ('york', 195467),\n",
              " ('doll', 46280),\n",
              " ('fac', 55669),\n",
              " ('and', 6634)]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Fit a TF-IDF vectorizer on the lemmatized words\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_vectorizer.fit(data.words_lemmatized.apply(\" \".join))\n",
        "\n",
        "print(\"Unique words:\", len(tfidf_vectorizer.vocabulary_))\n",
        "list(tfidf_vectorizer.vocabulary_.items())[:20]  # Show first 20 vocabulary entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiDCNx6fgM5D",
        "outputId": "84bbc690-41c7-4b33-a9aa-a3cdc10ebbe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of tweets: 159240\n",
            "Minimum sentence length: 1\n",
            "Maximum sentence length: 1383\n"
          ]
        }
      ],
      "source": [
        "# Filter out sentences with length 0\n",
        "filtered_sentences_with_indices = [(i, sentence) for i, sentence in enumerate(data.words_lemmatized.tolist()) if len(sentence) > 0]\n",
        "\n",
        "# Separate indices and sentences\n",
        "filtered_indices = [i for i, sentence in filtered_sentences_with_indices]\n",
        "sentences = [sentence for i, sentence in filtered_sentences_with_indices]\n",
        "\n",
        "# Check sentence stats after filtering\n",
        "print(\"Number of tweets:\", len(sentences))\n",
        "print(\"Minimum sentence length:\", min([len(sentence) for sentence in sentences]))\n",
        "print(\"Maximum sentence length:\", max([len(sentence) for sentence in sentences]))\n",
        "\n",
        "# Train a Word2Vec model\n",
        "w2v_model = gensim.models.Word2Vec(sentences=sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbZJ7q7CgM5D"
      },
      "outputs": [],
      "source": [
        "tagged_sentences = [gensim.models.doc2vec.TaggedDocument(sentence, [i]) for i, sentence in enumerate(sentences)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNsWPCeTgM5D"
      },
      "outputs": [],
      "source": [
        "d2v_model = gensim.models.Doc2Vec(\n",
        "    vector_size=100,     # Vector size (can be increased for better accuracy)\n",
        "    min_count=2,         # Minimum frequency of a word to be considered\n",
        "    epochs=40,           # Number of training epochs\n",
        "    window=5,            # Context window size\n",
        "    dm=1,                # Training algorithm: 1 — DM (Distributed Memory), 0 — DBOW\n",
        "    workers=4            # Number of worker threads (based on CPU cores)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2JObKyUgM5D",
        "outputId": "c129943a-0fd7-4e95-9e70-b63fdd1530e1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "159240"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "d2v_model.build_vocab(tagged_sentences)\n",
        "d2v_model.train(tagged_sentences, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)\n",
        "len(d2v_model.dv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rh26FnvigM5D",
        "outputId": "961a8d6c-a5c5-4caf-fc08-2dae4aa69cf9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>text</th>\n",
              "      <th>toxic</th>\n",
              "      <th>dirty_len</th>\n",
              "      <th>clean_len</th>\n",
              "      <th>words</th>\n",
              "      <th>word_count</th>\n",
              "      <th>word_count_cleaned</th>\n",
              "      <th>words_lemmatized</th>\n",
              "      <th>word_count_cleaned_no_digits</th>\n",
              "      <th>doc2vec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827</td>\n",
              "      <td>0</td>\n",
              "      <td>264</td>\n",
              "      <td>254</td>\n",
              "      <td>[Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]</td>\n",
              "      <td>43</td>\n",
              "      <td>31</td>\n",
              "      <td>[explanation, why, edits, make, username, hardcore, metallica, fan, revert, they, werent, vandalism, closure, gas, i, vote, new, york, doll, fac, and, please, dont, remove, template, talk, page, since, im, retire]</td>\n",
              "      <td>30</td>\n",
              "      <td>[0.7724078, -0.57992125, 0.8794413, -0.6368764, 0.04623737, 0.8891005, -0.7507112, -0.12640095, -0.72199714, -0.5170304, -0.47949797, -0.5531771, 1.0865148, -0.6857148, -0.45067087, 0.03693739, -0.60690266, 1.3406551, 0.2872252, 0.5704712, 0.6701598, -0.68028796, -0.04721625, -0.21645397, 0.5169594, 1.4363616, 0.28938785, 0.090483464, -0.74543, -0.5547359, -1.0830312, 0.02477289, 0.44779804, -0.6095908, 1.2240577, -0.5812384, 0.26844993, -0.23541619, 0.43634868, 0.83296686, -0.0072973557, 0.30824625, 0.39660963, -0.5157454, 0.25193092, -0.4177024, 0.40938038, -0.06400628, -1.0700427, -0.0031530897, 0.44634005, -0.03465753, -0.64062834, 0.5929856, 0.85718524, 0.3736478, 1.4557589, 0.68422997, -0.47857183, 0.044323076, 0.3513946, 0.35632926, -0.32313076, 1.054487, -0.11945534, -0.5186988, 0.09026859, 0.11671696, -0.2081145, 0.4887674, 0.4255189, 0.3209769, -0.021053392, -0.03933725, -0.8414647, 0.22005473, 0.63537025, -0.27483526, 0.065028064, 0.28864884, 0.13348518, -0.121625416, -0.9521123, -0.39673048, -0.048140228, 0.24405734, 0.47653645, 0.3309957, -0.12987736, 0.21896254, 0.42556113, -0.1648871, -0.66343623, 0.799902, 0.1954294, 0.46313673, 0.14548507, 0.050330408, 0.1994809, 0.13774525]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  \\\n",
              "0           0   \n",
              "\n",
              "                                                                                                                                                                                                                                                             text  \\\n",
              "0  Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827   \n",
              "\n",
              "   toxic  dirty_len  clean_len  \\\n",
              "0      0        264        254   \n",
              "\n",
              "                                                                                                                                                                                                                                       words  \\\n",
              "0  [Explanation, Why, edits, made, username, Hardcore, Metallica, Fan, reverted, They, werent, vandalisms, closure, GAs, I, voted, New, York, Dolls, FAC, And, please, dont, remove, template, talk, page, since, Im, retired, now892053827]   \n",
              "\n",
              "   word_count  word_count_cleaned  \\\n",
              "0          43                  31   \n",
              "\n",
              "                                                                                                                                                                                                        words_lemmatized  \\\n",
              "0  [explanation, why, edits, make, username, hardcore, metallica, fan, revert, they, werent, vandalism, closure, gas, i, vote, new, york, doll, fac, and, please, dont, remove, template, talk, page, since, im, retire]   \n",
              "\n",
              "   word_count_cleaned_no_digits  \\\n",
              "0                            30   \n",
              "\n",
              "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      doc2vec  \n",
              "0  [0.7724078, -0.57992125, 0.8794413, -0.6368764, 0.04623737, 0.8891005, -0.7507112, -0.12640095, -0.72199714, -0.5170304, -0.47949797, -0.5531771, 1.0865148, -0.6857148, -0.45067087, 0.03693739, -0.60690266, 1.3406551, 0.2872252, 0.5704712, 0.6701598, -0.68028796, -0.04721625, -0.21645397, 0.5169594, 1.4363616, 0.28938785, 0.090483464, -0.74543, -0.5547359, -1.0830312, 0.02477289, 0.44779804, -0.6095908, 1.2240577, -0.5812384, 0.26844993, -0.23541619, 0.43634868, 0.83296686, -0.0072973557, 0.30824625, 0.39660963, -0.5157454, 0.25193092, -0.4177024, 0.40938038, -0.06400628, -1.0700427, -0.0031530897, 0.44634005, -0.03465753, -0.64062834, 0.5929856, 0.85718524, 0.3736478, 1.4557589, 0.68422997, -0.47857183, 0.044323076, 0.3513946, 0.35632926, -0.32313076, 1.054487, -0.11945534, -0.5186988, 0.09026859, 0.11671696, -0.2081145, 0.4887674, 0.4255189, 0.3209769, -0.021053392, -0.03933725, -0.8414647, 0.22005473, 0.63537025, -0.27483526, 0.065028064, 0.28864884, 0.13348518, -0.121625416, -0.9521123, -0.39673048, -0.048140228, 0.24405734, 0.47653645, 0.3309957, -0.12987736, 0.21896254, 0.42556113, -0.1648871, -0.66343623, 0.799902, 0.1954294, 0.46313673, 0.14548507, 0.050330408, 0.1994809, 0.13774525]  "
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data['doc2vec'] = None\n",
        "for idx, vector in zip(filtered_indices, d2v_model.dv):\n",
        "    data.at[idx, 'doc2vec'] = vector\n",
        "data.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aERYemUAgM5E"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxbLmzUfMD0s"
      },
      "outputs": [],
      "source": [
        "%free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G29w0yBfMZ9y"
      },
      "outputs": [],
      "source": [
        "%sync; echo 3 > /proc/sys/vm/drop_caches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-M8FFkl-MdWS"
      },
      "outputs": [],
      "source": [
        "%free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lxU_3YBgM5E",
        "outputId": "4827a06a-70ba-4339-9b98-dc31c1071ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 159292 entries, 0 to 159291\n",
            "Data columns (total 11 columns):\n",
            " #   Column                        Non-Null Count   Dtype \n",
            "---  ------                        --------------   ----- \n",
            " 0   Unnamed: 0                    159292 non-null  int64 \n",
            " 1   text                          159292 non-null  object\n",
            " 2   toxic                         159292 non-null  int64 \n",
            " 3   dirty_len                     159292 non-null  int64 \n",
            " 4   clean_len                     159292 non-null  int64 \n",
            " 5   words                         159292 non-null  object\n",
            " 6   word_count                    159292 non-null  int64 \n",
            " 7   word_count_cleaned            159292 non-null  int64 \n",
            " 8   words_lemmatized              159292 non-null  object\n",
            " 9   word_count_cleaned_no_digits  159292 non-null  int64 \n",
            " 10  doc2vec                       159240 non-null  object\n",
            "dtypes: int64(7), object(4)\n",
            "memory usage: 13.4+ MB\n"
          ]
        }
      ],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "CchfycwtgM5E",
        "outputId": "0477a2a6-d948-43e3-a323-efe982a8f3dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0                                                                                                                                                                                                                                                                                                                                                          Explanation Why the edits made under my username Hardcore Metallica Fan were reverted They werent vandalisms just closure on some GAs after I voted at New York Dolls FAC And please dont remove the template from the talk page since Im retired now892053827\n",
            "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Daww He matches this background colour Im seemingly stuck with Thanks talk 2151 January 11 2016 UTC\n",
            "2                                                                                                                                                                                                                                                                                                                                                                                     Hey man Im really not trying to edit war Its just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page He seems to care more about the formatting than the actual info\n",
            "3     More I cant make any real suggestions on improvement I wondered if the section statistics should be later on or a subsection of types of accidents I think the references may need tidying so that they are all in the exact same format ie date format etc I can do that later on if noone else does first if you have any preferences for formatting style on references or want to do it yourself please let me know There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up Its listed in the relevant form eg WikipediaGoodarticlenominationsTransport \n",
            "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          You sir are my hero Any chance you remember what page thats on\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "# Extract features and target variable\n",
        "y = data['toxic']  # Use the 'toxic' column as the target variable\n",
        "\n",
        "# Feature matrix - use text column for TF-IDF vectorization\n",
        "X = data['text']  # Use the 'text' column as input features\n",
        "\n",
        "print(X.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "ttsfAW6dNyx1",
        "outputId": "dc9e8f5a-4acf-41f6-ee25-f8adfd33d107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    0\n",
            "1    0\n",
            "2    0\n",
            "3    0\n",
            "4    0\n",
            "Name: toxic, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W__imbLWOrd1",
        "outputId": "b60d08ed-151a-4030-e080-9a04bfaa4c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets are ready\n"
          ]
        }
      ],
      "source": [
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print('Datasets are ready')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6-rDSn6ftTI",
        "outputId": "700de37f-dec7-4224-982c-1e7252e6f2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unnamed: 0                       0\n",
            "text                             0\n",
            "toxic                            0\n",
            "dirty_len                        0\n",
            "clean_len                        0\n",
            "words                            0\n",
            "word_count                       0\n",
            "word_count_cleaned               0\n",
            "words_lemmatized                 0\n",
            "word_count_cleaned_no_digits     0\n",
            "doc2vec                         52\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(data.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmZ0rGorslb3"
      },
      "outputs": [],
      "source": [
        "data_cleaned = data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPzjtpixtAuU",
        "outputId": "4da8ae2e-35c6-4d49-c3c9-1c326ffd4121"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 159240 entries, 0 to 159291\n",
            "Data columns (total 11 columns):\n",
            " #   Column                        Non-Null Count   Dtype \n",
            "---  ------                        --------------   ----- \n",
            " 0   Unnamed: 0                    159240 non-null  int64 \n",
            " 1   text                          159240 non-null  object\n",
            " 2   toxic                         159240 non-null  int64 \n",
            " 3   dirty_len                     159240 non-null  int64 \n",
            " 4   clean_len                     159240 non-null  int64 \n",
            " 5   words                         159240 non-null  object\n",
            " 6   word_count                    159240 non-null  int64 \n",
            " 7   word_count_cleaned            159240 non-null  int64 \n",
            " 8   words_lemmatized              159240 non-null  object\n",
            " 9   word_count_cleaned_no_digits  159240 non-null  int64 \n",
            " 10  doc2vec                       159240 non-null  object\n",
            "dtypes: int64(7), object(4)\n",
            "memory usage: 14.6+ MB\n"
          ]
        }
      ],
      "source": [
        "data_cleaned.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtiIIDfQszuK"
      },
      "outputs": [],
      "source": [
        "y_cleaned = data_cleaned['toxic']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-adA-jEtHa2",
        "outputId": "183fef47-4faa-477f-8a07-87af0398ec9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "Index: 159240 entries, 0 to 159291\n",
            "Series name: toxic\n",
            "Non-Null Count   Dtype\n",
            "--------------   -----\n",
            "159240 non-null  int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 2.4 MB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(y_cleaned.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ban-FPPk2BPx"
      },
      "outputs": [],
      "source": [
        "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(\n",
        "    data_cleaned, y_cleaned, test_size=0.2, random_state=42, stratify=y_cleaned\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hbalMVe1x26",
        "outputId": "b845ca8d-0979-4687-a2a9-b1d4d59fc73e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set shape: (127392, 213275)\n",
            "Test set shape: (31848, 213275)\n"
          ]
        }
      ],
      "source": [
        "# 2. Convert text into TF-IDF features\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform on the training set\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_full['text'])\n",
        "\n",
        "# Transform the test set using the same vectorizer\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test_full['text'])\n",
        "\n",
        "# 3. Transform numerical features using FunctionTransformer\n",
        "def numeric_transformer(X):\n",
        "    return csr_matrix(X[['dirty_len', 'clean_len', 'word_count', 'word_count_cleaned', 'word_count_cleaned_no_digits']].values)\n",
        "\n",
        "numeric_features = FunctionTransformer(numeric_transformer)\n",
        "\n",
        "# Transform the training set\n",
        "X_train_numeric = numeric_features.transform(X_train_full)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_numeric = numeric_features.transform(X_test_full)\n",
        "\n",
        "# 4. Convert Doc2Vec vectors into NumPy arrays, then to sparse matrix\n",
        "def doc2vec_transformer(X):\n",
        "    return csr_matrix(np.array([np.zeros(100) if vec is None else vec for vec in X['doc2vec']]))\n",
        "\n",
        "doc2vec_features = FunctionTransformer(doc2vec_transformer)\n",
        "\n",
        "# Transform the training set\n",
        "X_train_doc2vec = doc2vec_features.transform(X_train_full)\n",
        "\n",
        "# Transform the test set\n",
        "X_test_doc2vec = doc2vec_features.transform(X_test_full)\n",
        "\n",
        "# 5. Combine all features into a single matrix for the training set\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_numeric, X_train_doc2vec])\n",
        "\n",
        "# Combine all features into a single matrix for the test set\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_numeric, X_test_doc2vec])\n",
        "\n",
        "print(f\"Training set shape: {X_train_combined.shape}\")\n",
        "print(f\"Test set shape: {X_test_combined.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0l3aUeTcgM5E"
      },
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9e7oKkuQHw4",
        "outputId": "8fbfb593-b768-4e69-dc40-e17d5ec2a03e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
            "Best parameters: {'model__C': 10, 'model__penalty': 'l2', 'model__solver': 'liblinear'}\n",
            "Best F1-score: 0.7653781905047358\n"
          ]
        }
      ],
      "source": [
        "# 1. List of stop words\n",
        "stop_words = list(stopwords.words('english'))\n",
        "\n",
        "# 2. Create a pipeline that includes a TF-IDF vectorizer and a logistic regression model\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(\n",
        "        stop_words=stop_words,     # Use the list of stop words\n",
        "        ngram_range=(1, 2),        # Use unigrams and bigrams\n",
        "        min_df=5,                  # Ignore terms that appear in fewer than 5 documents\n",
        "        max_df=0.9                 # Ignore terms that appear in more than 90% of documents\n",
        "    )),\n",
        "    ('model', LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced',\n",
        "        solver='liblinear'\n",
        "    ))\n",
        "])\n",
        "\n",
        "# 3. Hyperparameter tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'model__C': [1, 10, 20],                # Regularization strength for logistic regression\n",
        "    'model__penalty': ['l1', 'l2'],         # L1 and L2 regularization\n",
        "    'model__solver': ['liblinear']          # Solver that supports both L1 and L2\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',        # Evaluation metric: F1-score\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 4. Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Save the best model to a variable\n",
        "best_model_reg = grid_search.best_estimator_\n",
        "\n",
        "# 6. Best model results\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best F1-score:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYmHqtPyV7k6"
      },
      "source": [
        "The best F1-score on cross-validation is 0.76, which meets the task requirements. But let's also try logistic regression on the dataset with additional features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uwiy2YWgUvpw"
      },
      "outputs": [],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BJF71BqVQwb",
        "outputId": "63639a3a-d242-4032-bd1e-6a6bd35a1e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "full_text: 239.8 MiB\n",
            "data_cleaned: 216.2 MiB\n",
            "data: 215.0 MiB\n",
            "X_train_full: 173.0 MiB\n",
            "X: 65.4 MiB\n",
            "X_train: 53.3 MiB\n",
            "all_words: 47.2 MiB\n",
            "all_words_lemmatized: 47.2 MiB\n",
            "X_test_full: 43.2 MiB\n",
            "X_test: 13.3 MiB\n",
            "y_cleaned: 2.4 MiB\n",
            "y_train: 1.9 MiB\n",
            "y_train_full: 1.9 MiB\n",
            "filtered_sentences_with_indices: 1.2 MiB\n",
            "filtered_indices: 1.2 MiB\n",
            "sentences: 1.2 MiB\n",
            "tagged_sentences: 1.2 MiB\n",
            "y: 1.2 MiB\n",
            "y_test: 497.8 KiB\n",
            "y_test_full: 497.6 KiB\n",
            "over_127: 236.0 KiB\n",
            "data_symbols: 225.6 KiB\n",
            "data_symbols_cleaned: 224.6 KiB\n",
            "counter: 72.1 KiB\n",
            "translation_table: 72.1 KiB\n",
            "bad_chars: 8.9 KiB\n",
            "TfidfVectorizer: 2.0 KiB\n",
            "stop_words: 1.6 KiB\n",
            "GridSearchCV: 1.4 KiB\n",
            "LogisticRegression: 1.4 KiB\n",
            "FunctionTransformer: 1.2 KiB\n",
            "Dictionary: 1.2 KiB\n",
            "WordNetLemmatizer: 1.0 KiB\n",
            "Pipeline: 1.0 KiB\n",
            "FeatureUnion: 1.0 KiB\n",
            "Counter: 1.0 KiB\n",
            "TaggedDocument: 1.0 KiB\n",
            "BertTokenizer: 1.0 KiB\n",
            "BertConfig: 1.0 KiB\n",
            "BertModel: 1.0 KiB\n",
            "csr_matrix: 896.0 B\n",
            "In: 664.0 B\n",
            "Out: 640.0 B\n",
            "datetime: 408.0 B\n",
            "param_grid: 232.0 B\n",
            "open: 136.0 B\n",
            "pos_tag: 136.0 B\n",
            "cosine_similarity: 136.0 B\n",
            "train_test_split: 136.0 B\n",
            "cross_val_score: 136.0 B\n",
            "f1_score: 136.0 B\n",
            "make_scorer: 136.0 B\n",
            "datapath: 136.0 B\n",
            "hstack: 136.0 B\n",
            "replace_whitespace: 136.0 B\n",
            "get_symbol_counter: 136.0 B\n",
            "translate_text: 136.0 B\n",
            "check_multiple_whitespaces: 136.0 B\n",
            "remove_multiple_whitespaces: 136.0 B\n",
            "split_text: 136.0 B\n",
            "remove_stop_words: 136.0 B\n",
            "get_wordnet_pos: 136.0 B\n",
            "lemmatize_word_list: 136.0 B\n",
            "preprocess_text: 136.0 B\n",
            "remove_digits: 136.0 B\n",
            "remove_words_with_digits: 136.0 B\n",
            "numeric_transformer: 136.0 B\n",
            "doc2vec_transformer: 136.0 B\n",
            "sizeof_fmt: 136.0 B\n",
            "vector: 112.0 B\n",
            "np: 72.0 B\n",
            "pd: 72.0 B\n",
            "nltk: 72.0 B\n",
            "plt: 72.0 B\n",
            "sns: 72.0 B\n",
            "re: 72.0 B\n",
            "string: 72.0 B\n",
            "itertools: 72.0 B\n",
            "contractions: 72.0 B\n",
            "gensim: 72.0 B\n",
            "torch: 72.0 B\n",
            "transformers: 72.0 B\n",
            "notebook: 72.0 B\n",
            "warnings: 72.0 B\n",
            "gc: 72.0 B\n",
            "sys: 72.0 B\n",
            "os: 72.0 B\n",
            "get_ipython: 64.0 B\n",
            "exit: 48.0 B\n",
            "quit: 48.0 B\n",
            "stopwords: 48.0 B\n",
            "wordnet: 48.0 B\n",
            "lemmatizer: 48.0 B\n",
            "tfidf_vectorizer: 48.0 B\n",
            "w2v_model: 48.0 B\n",
            "d2v_model: 48.0 B\n",
            "X_train_tfidf: 48.0 B\n",
            "X_test_tfidf: 48.0 B\n",
            "numeric_features: 48.0 B\n",
            "X_train_numeric: 48.0 B\n",
            "X_test_numeric: 48.0 B\n",
            "doc2vec_features: 48.0 B\n",
            "X_train_doc2vec: 48.0 B\n",
            "X_test_doc2vec: 48.0 B\n",
            "X_train_combined: 48.0 B\n",
            "X_test_combined: 48.0 B\n",
            "pipeline: 48.0 B\n",
            "grid_search: 48.0 B\n",
            "best_model_reg: 48.0 B\n",
            "idx: 28.0 B\n"
          ]
        }
      ],
      "source": [
        "# Function to format the size of an object in a more readable way\n",
        "def sizeof_fmt(num, suffix='B'):\n",
        "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
        "        if abs(num) < 1024.0:\n",
        "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
        "        num /= 1024.0\n",
        "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
        "\n",
        "# Get all variables in the global scope\n",
        "all_variables = {name: sys.getsizeof(value) for name, value in globals().items() if not name.startswith('_')}\n",
        "\n",
        "# Sort variables by size in descending order\n",
        "sorted_variables = sorted(all_variables.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Print the sizes of the variables\n",
        "for name, size in sorted_variables:\n",
        "    print(f\"{name}: {sizeof_fmt(size)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Wdj0wntVsa3",
        "outputId": "6140a273-0b08-4524-f0aa-603438cd1de4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "\n",
        "# Удаление ненужных переменных\n",
        "del full_text\n",
        "del all_words\n",
        "del all_words_lemmatized\n",
        "del filtered_sentences_with_indices\n",
        "del filtered_indices\n",
        "del sentences\n",
        "del tagged_sentences\n",
        "del over_127\n",
        "del data_symbols\n",
        "del data_symbols_cleaned\n",
        "del translation_table\n",
        "del bad_chars\n",
        "del all_variables\n",
        "del pos_tag\n",
        "del replace_whitespace\n",
        "del get_symbol_counter\n",
        "del translate_text\n",
        "del check_multiple_whitespaces\n",
        "del remove_multiple_whitespaces\n",
        "del split_text\n",
        "del remove_stop_words\n",
        "del get_wordnet_pos\n",
        "del lemmatize_word_list\n",
        "del preprocess_text\n",
        "del remove_digits\n",
        "del remove_words_with_digits\n",
        "\n",
        "# Вызов сборщика мусора для очистки памяти\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3GIwd8hXtih"
      },
      "outputs": [],
      "source": [
        "!free -h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0M8xvaZFgM5G"
      },
      "source": [
        "### Logistic Regresion with Additional Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKkVVikggM5G"
      },
      "source": [
        "I'll try using the features I created during the text preprocessing process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUz_SzZzgM5H",
        "outputId": "5b2c32a7-0cd7-4e92-ae84-cd315bf9be92",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "[CV 1/5] END .C=1, penalty=l2, solver=liblinear;, score=0.743 total time= 4.3min\n",
            "[CV 3/5] END .C=1, penalty=l2, solver=liblinear;, score=0.757 total time= 6.1min\n",
            "[CV 4/5] END .C=1, penalty=l2, solver=liblinear;, score=0.749 total time= 7.2min\n",
            "[CV 2/5] END .C=1, penalty=l2, solver=liblinear;, score=0.739 total time= 8.4min\n",
            "[CV 2/5] END C=10, penalty=l2, solver=liblinear;, score=0.771 total time= 8.8min\n",
            "[CV 5/5] END .C=1, penalty=l2, solver=liblinear;, score=0.738 total time= 9.5min\n",
            "[CV 3/5] END C=10, penalty=l2, solver=liblinear;, score=0.785 total time=11.7min\n",
            "[CV 1/5] END C=10, penalty=l2, solver=liblinear;, score=0.782 total time=13.5min\n",
            "[CV 4/5] END C=10, penalty=l2, solver=liblinear;, score=0.785 total time= 9.5min\n",
            "[CV 1/5] END C=20, penalty=l2, solver=liblinear;, score=0.785 total time= 6.7min\n",
            "[CV 2/5] END C=20, penalty=l2, solver=liblinear;, score=0.772 total time= 5.6min\n",
            "[CV 5/5] END C=10, penalty=l2, solver=liblinear;, score=0.774 total time= 8.7min\n",
            "[CV 3/5] END C=20, penalty=l2, solver=liblinear;, score=0.790 total time= 6.3min\n",
            "[CV 4/5] END C=20, penalty=l2, solver=liblinear;, score=0.785 total time= 6.0min\n",
            "[CV 5/5] END C=20, penalty=l2, solver=liblinear;, score=0.777 total time= 4.2min\n",
            "Best parameters: {'C': 20, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "Best F1-score: 0.7817843067511727\n"
          ]
        }
      ],
      "source": [
        "# Define logistic regression model\n",
        "model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    solver='liblinear'\n",
        ")\n",
        "\n",
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [1, 10, 20],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        "    verbose=3\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train_combined, y_train_full)\n",
        "\n",
        "# Save the best model\n",
        "best_model_reg_full = grid_search.best_estimator_\n",
        "\n",
        "# Best model results\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best F1-score:\", grid_search.best_score_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgpvCl77LArD"
      },
      "source": [
        "The regression algorithm with additional features achieved an F1-score of 0.78 on cross-validation, which is better than without the features and meets the task requirements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Njft7ldWgM5I"
      },
      "source": [
        "### BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG5XTWOWJMjk"
      },
      "outputs": [],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('unitary/toxic-bert')\n",
        "\n",
        "MAX_LEN = 512\n",
        "\n",
        "# Tokenize the text, adding special tokens, truncating, and padding automatically\n",
        "encoding = tokenizer(\n",
        "    data['text'].tolist(),\n",
        "    add_special_tokens=True,  # Add [CLS] and [SEP] tokens\n",
        "    max_length=MAX_LEN,       # Maximum length for truncation\n",
        "    truncation=True,          # Truncate sequences longer than MAX_LEN\n",
        "    padding='longest',        # Pad sequences to the longest in the batch\n",
        "    return_tensors='pt',      # Return as PyTorch tensors\n",
        "    return_attention_mask=True  # Return attention mask as well\n",
        ")\n",
        "\n",
        "# Extract input_ids and attention_mask from the tokenized output\n",
        "input_ids = encoding['input_ids']\n",
        "attention_mask = encoding['attention_mask']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh_d0cmwHqQ-"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDiPR6KdgM5J"
      },
      "outputs": [],
      "source": [
        "config = BertConfig.from_pretrained('unitary/toxic-bert')\n",
        "model = BertModel.from_pretrained('unitary/toxic-bert', config=config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mOhBdI7IF9Z",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "# Assuming input_ids and attention_mask are defined\n",
        "batch_size = 128  # Example batch size\n",
        "embeddings = []\n",
        "\n",
        "# Move the model to the GPU\n",
        "model = model.to(device)\n",
        "\n",
        "# Iterate over the dataset in batches\n",
        "for i in notebook.tqdm(range(input_ids.shape[0] // batch_size)):\n",
        "    # Move the batch to GPU\n",
        "    batch = torch.tensor(input_ids[batch_size * i:batch_size * (i + 1)], dtype=torch.long).to(device)\n",
        "    attention_mask_batch = torch.tensor(attention_mask[batch_size * i:batch_size * (i + 1)], dtype=torch.long).to(device)\n",
        "\n",
        "    # Disable gradient computation for inference\n",
        "    with torch.no_grad():\n",
        "        # Get the embeddings\n",
        "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
        "\n",
        "    # Extract [CLS] token embeddings and move them to CPU, then convert to NumPy\n",
        "    embeddings.append(batch_embeddings.last_hidden_state[:, 0, :].cpu().numpy())\n",
        "\n",
        "    # Free up GPU memory\n",
        "    del batch\n",
        "    del attention_mask_batch\n",
        "    del batch_embeddings\n",
        "    torch.cuda.empty_cache()  # Clear GPU cache to free memory\n",
        "\n",
        "# Concatenate the embeddings into a single NumPy array\n",
        "features = np.concatenate(embeddings, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DplFZ74CgM5K"
      },
      "outputs": [],
      "source": [
        "# Get the target labels\n",
        "selected_labels = data['toxic'].values\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_bert, X_test_bert, y_train_bert, y_test_bert = train_test_split(\n",
        "    features, selected_labels, test_size=0.2, random_state=42, stratify=selected_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFbGJFaAP1K9"
      },
      "outputs": [],
      "source": [
        "# Train a logistic regression model\n",
        "lr_model = LogisticRegression(\n",
        "    random_state=42,\n",
        "    max_iter=1000,\n",
        "    class_weight='balanced',\n",
        "    solver='liblinear'\n",
        ")\n",
        "\n",
        "lr_model.fit(X_train_bert, y_train_bert)\n",
        "\n",
        "y_train_pred = lr_model.predict(X_train_bert)\n",
        "\n",
        "f1_scorer = make_scorer(f1_score)\n",
        "\n",
        "# Cross-validation with F1 scoring\n",
        "f1_scores = cross_val_score(lr_model, X_train_bert, y_train_bert, cv=5, scoring=f1_scorer)\n",
        "\n",
        "print(f\"F1-score after cross-validation: {f1_scores.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baNOMDAkRb1J"
      },
      "source": [
        "BERT accuracy on the training set is 0.89 — this is the best result so far. Let's test it on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BouVHxM600gV"
      },
      "source": [
        "## Evaluation of the Best Model on the Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjX0gLAZgM5K"
      },
      "outputs": [],
      "source": [
        "\n",
        "y_test_pred = lr_model.predict(X_test_bert)\n",
        "f1_bert_test = f1_score(y_test_bert, y_test_pred)\n",
        "\n",
        "print(f\"F1 Score on the Test Set: {f1_bert_test:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3Fr76kIUovu"
      },
      "source": [
        "The BERT model’s F1 score on the test set is 0.88, which meets the task requirements. The model correctly identifies a high percentage of both true positive and true negative toxic examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TcwvRf-gM5K"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdnRjNFJgM5K"
      },
      "source": [
        "The online store Wikishop is launching a new service that allows users to edit and supplement product descriptions, similar to wiki communities. In other words, customers can suggest edits and comment on changes made by others. The store needs a tool that can detect toxic comments and send them for moderation.\n",
        "\n",
        "The task was to train a model to classify comments as positive or negative, and evaluate it using quality metrics — specifically, the F1 score had to be at least 0.75.\n",
        "\n",
        "I was provided with a dataset containing comment texts and their labels (toxic or not). I performed text preprocessing: removed extra spaces, stopwords, and digits; lemmatized the text; and created vector representations for each message. I preserved all transformations as additional features.\n",
        "\n",
        "I trained a logistic regression model on the original dataset and the one with additional features, and measured F1 scores on the validation set. I also tested a model based on BERT and evaluated it using 5-fold cross-validation.\n",
        "\n",
        "*Results*:\n",
        "- Original dataset: F1 = 0.76\n",
        "\n",
        "- Dataset with additional features: F1 = 0.78\n",
        "\n",
        "- BERT: F1 = 0.89\n",
        "\n",
        "In conclusion, the best F1 score was achieved by a logistic regression model using embeddings generated by the BERT model, with an F1 score of 0.88 on the test set.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "ExecuteTimeLog": [
      {
        "duration": 1939,
        "start_time": "2024-10-12T13:25:42.415Z"
      },
      {
        "duration": 22649,
        "start_time": "2024-10-12T13:26:18.772Z"
      },
      {
        "duration": 3172,
        "start_time": "2024-10-12T13:26:41.423Z"
      },
      {
        "duration": 3642,
        "start_time": "2024-10-12T13:27:31.100Z"
      },
      {
        "duration": 3247,
        "start_time": "2024-10-12T13:28:31.301Z"
      },
      {
        "duration": 4681,
        "start_time": "2024-10-12T13:28:48.288Z"
      },
      {
        "duration": 6306,
        "start_time": "2024-10-12T13:29:56.360Z"
      },
      {
        "duration": 8441,
        "start_time": "2024-10-12T13:31:08.336Z"
      },
      {
        "duration": 2537,
        "start_time": "2024-10-12T13:31:35.259Z"
      },
      {
        "duration": 2567,
        "start_time": "2024-10-12T13:32:15.589Z"
      },
      {
        "duration": 2421,
        "start_time": "2024-10-12T13:32:28.603Z"
      },
      {
        "duration": 4505,
        "start_time": "2024-10-12T13:33:25.140Z"
      },
      {
        "duration": 4635,
        "start_time": "2024-10-12T13:36:00.191Z"
      },
      {
        "duration": 0,
        "start_time": "2024-10-12T13:41:17.503Z"
      },
      {
        "duration": 0,
        "start_time": "2024-10-12T13:41:17.504Z"
      },
      {
        "duration": 0,
        "start_time": "2024-10-12T13:41:17.506Z"
      },
      {
        "duration": 5889,
        "start_time": "2024-10-12T13:41:29.912Z"
      },
      {
        "duration": 2445,
        "start_time": "2024-10-12T13:42:08.625Z"
      },
      {
        "duration": 6,
        "start_time": "2024-10-12T13:42:17.176Z"
      },
      {
        "duration": 878,
        "start_time": "2024-10-12T13:42:24.832Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T13:43:35.352Z"
      },
      {
        "duration": 31,
        "start_time": "2024-10-12T13:43:37.798Z"
      },
      {
        "duration": 269,
        "start_time": "2024-10-12T13:43:41.905Z"
      },
      {
        "duration": 188,
        "start_time": "2024-10-12T13:43:44.653Z"
      },
      {
        "duration": 3111,
        "start_time": "2024-10-12T13:43:54.286Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T13:44:00.198Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T13:44:06.926Z"
      },
      {
        "duration": 3439,
        "start_time": "2024-10-12T13:44:11.146Z"
      },
      {
        "duration": 6,
        "start_time": "2024-10-12T13:44:19.292Z"
      },
      {
        "duration": 3325,
        "start_time": "2024-10-12T13:44:27.671Z"
      },
      {
        "duration": 210,
        "start_time": "2024-10-12T13:44:39.087Z"
      },
      {
        "duration": 196,
        "start_time": "2024-10-12T13:44:43.245Z"
      },
      {
        "duration": 4629,
        "start_time": "2024-10-12T13:45:46.606Z"
      },
      {
        "duration": 198,
        "start_time": "2024-10-12T13:46:38.580Z"
      },
      {
        "duration": 200,
        "start_time": "2024-10-12T13:47:38.544Z"
      },
      {
        "duration": 228,
        "start_time": "2024-10-12T13:47:42.850Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T13:48:16.025Z"
      },
      {
        "duration": 6,
        "start_time": "2024-10-12T13:48:19.568Z"
      },
      {
        "duration": 105,
        "start_time": "2024-10-12T13:48:24.251Z"
      },
      {
        "duration": 4533,
        "start_time": "2024-10-12T13:48:27.200Z"
      },
      {
        "duration": 79,
        "start_time": "2024-10-12T13:48:33.974Z"
      },
      {
        "duration": 1015,
        "start_time": "2024-10-12T13:48:38.097Z"
      },
      {
        "duration": 2735,
        "start_time": "2024-10-12T13:48:42.738Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T13:48:48.076Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T13:48:56.481Z"
      },
      {
        "duration": 2008,
        "start_time": "2024-10-12T13:49:00.669Z"
      },
      {
        "duration": 11,
        "start_time": "2024-10-12T13:49:05.991Z"
      },
      {
        "duration": 75,
        "start_time": "2024-10-12T13:49:16.345Z"
      },
      {
        "duration": 1936,
        "start_time": "2024-10-12T13:49:23.887Z"
      },
      {
        "duration": 5,
        "start_time": "2024-10-12T13:49:41.237Z"
      },
      {
        "duration": 6,
        "start_time": "2024-10-12T13:51:48.605Z"
      },
      {
        "duration": 331832,
        "start_time": "2024-10-12T13:52:55.113Z"
      },
      {
        "duration": 63,
        "start_time": "2024-10-12T14:02:17.525Z"
      },
      {
        "duration": 261,
        "start_time": "2024-10-12T14:02:23.037Z"
      },
      {
        "duration": 21,
        "start_time": "2024-10-12T14:02:48.208Z"
      },
      {
        "duration": 3466,
        "start_time": "2024-10-12T14:03:09.587Z"
      },
      {
        "duration": 21,
        "start_time": "2024-10-12T14:03:18.699Z"
      },
      {
        "duration": 302,
        "start_time": "2024-10-12T14:03:40.744Z"
      },
      {
        "duration": 10,
        "start_time": "2024-10-12T14:04:55.116Z"
      },
      {
        "duration": 279,
        "start_time": "2024-10-12T14:04:59.924Z"
      },
      {
        "duration": 10,
        "start_time": "2024-10-12T14:06:09.096Z"
      },
      {
        "duration": 49,
        "start_time": "2024-10-12T14:06:12.427Z"
      },
      {
        "duration": 295,
        "start_time": "2024-10-12T14:07:49.961Z"
      },
      {
        "duration": 41,
        "start_time": "2024-10-12T14:08:50.565Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T14:10:29.808Z"
      },
      {
        "duration": 42,
        "start_time": "2024-10-12T14:10:33.410Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T14:12:09.664Z"
      },
      {
        "duration": 43,
        "start_time": "2024-10-12T14:12:13.682Z"
      },
      {
        "duration": 4567,
        "start_time": "2024-10-12T14:12:56.422Z"
      },
      {
        "duration": 901,
        "start_time": "2024-10-12T14:13:09.840Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T14:13:18.579Z"
      },
      {
        "duration": 28,
        "start_time": "2024-10-12T14:13:20.813Z"
      },
      {
        "duration": 292,
        "start_time": "2024-10-12T14:13:23.176Z"
      },
      {
        "duration": 188,
        "start_time": "2024-10-12T14:13:24.964Z"
      },
      {
        "duration": 3350,
        "start_time": "2024-10-12T14:13:29.694Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T14:13:34.736Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T14:13:37.339Z"
      },
      {
        "duration": 3463,
        "start_time": "2024-10-12T14:13:39.916Z"
      },
      {
        "duration": 6,
        "start_time": "2024-10-12T14:13:46.642Z"
      },
      {
        "duration": 3388,
        "start_time": "2024-10-12T14:13:49.445Z"
      },
      {
        "duration": 198,
        "start_time": "2024-10-12T14:13:55.437Z"
      },
      {
        "duration": 198,
        "start_time": "2024-10-12T14:13:59.617Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T14:14:03.545Z"
      },
      {
        "duration": 5,
        "start_time": "2024-10-12T14:14:06.090Z"
      },
      {
        "duration": 71,
        "start_time": "2024-10-12T14:14:09.094Z"
      },
      {
        "duration": 4435,
        "start_time": "2024-10-12T14:14:12.714Z"
      },
      {
        "duration": 66,
        "start_time": "2024-10-12T14:14:19.836Z"
      },
      {
        "duration": 1095,
        "start_time": "2024-10-12T14:14:22.481Z"
      },
      {
        "duration": 2744,
        "start_time": "2024-10-12T14:14:25.877Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T14:14:29.105Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T14:14:33.290Z"
      },
      {
        "duration": 1496,
        "start_time": "2024-10-12T14:14:35.234Z"
      },
      {
        "duration": 11,
        "start_time": "2024-10-12T14:14:38.426Z"
      },
      {
        "duration": 76,
        "start_time": "2024-10-12T14:14:41.527Z"
      },
      {
        "duration": 2434,
        "start_time": "2024-10-12T14:14:50.800Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T14:14:57.503Z"
      },
      {
        "duration": 108,
        "start_time": "2024-10-12T14:15:08.566Z"
      },
      {
        "duration": 41,
        "start_time": "2024-10-12T14:15:11.291Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T14:41:53.749Z"
      },
      {
        "duration": 42,
        "start_time": "2024-10-12T14:41:56.989Z"
      },
      {
        "duration": 39,
        "start_time": "2024-10-12T14:42:14.115Z"
      },
      {
        "duration": 39,
        "start_time": "2024-10-12T14:42:23.795Z"
      },
      {
        "duration": 11476,
        "start_time": "2024-10-12T14:42:50.540Z"
      },
      {
        "duration": 1426,
        "start_time": "2024-10-12T14:43:48.746Z"
      },
      {
        "duration": 5744,
        "start_time": "2024-10-12T14:43:50.174Z"
      },
      {
        "duration": 2589,
        "start_time": "2024-10-12T14:43:55.920Z"
      },
      {
        "duration": 2548,
        "start_time": "2024-10-12T14:43:58.511Z"
      },
      {
        "duration": 4619,
        "start_time": "2024-10-12T14:44:01.060Z"
      },
      {
        "duration": 893,
        "start_time": "2024-10-12T14:44:05.680Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T14:44:06.574Z"
      },
      {
        "duration": 42,
        "start_time": "2024-10-12T14:44:06.579Z"
      },
      {
        "duration": 281,
        "start_time": "2024-10-12T14:44:06.622Z"
      },
      {
        "duration": 202,
        "start_time": "2024-10-12T14:44:06.905Z"
      },
      {
        "duration": 3074,
        "start_time": "2024-10-12T14:44:07.109Z"
      },
      {
        "duration": 24,
        "start_time": "2024-10-12T14:44:10.184Z"
      },
      {
        "duration": 14,
        "start_time": "2024-10-12T14:44:10.209Z"
      },
      {
        "duration": 3420,
        "start_time": "2024-10-12T14:44:10.224Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T14:44:13.645Z"
      },
      {
        "duration": 3351,
        "start_time": "2024-10-12T14:44:13.654Z"
      },
      {
        "duration": 205,
        "start_time": "2024-10-12T14:44:17.006Z"
      },
      {
        "duration": 241,
        "start_time": "2024-10-12T14:44:17.212Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T14:44:17.454Z"
      },
      {
        "duration": 32,
        "start_time": "2024-10-12T14:44:17.462Z"
      },
      {
        "duration": 81,
        "start_time": "2024-10-12T14:44:17.496Z"
      },
      {
        "duration": 4329,
        "start_time": "2024-10-12T14:44:17.579Z"
      },
      {
        "duration": 73,
        "start_time": "2024-10-12T14:44:21.910Z"
      },
      {
        "duration": 1062,
        "start_time": "2024-10-12T14:44:21.985Z"
      },
      {
        "duration": 2710,
        "start_time": "2024-10-12T14:44:23.048Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T14:44:25.760Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T14:44:25.769Z"
      },
      {
        "duration": 1533,
        "start_time": "2024-10-12T14:44:25.776Z"
      },
      {
        "duration": 17,
        "start_time": "2024-10-12T14:44:27.311Z"
      },
      {
        "duration": 98,
        "start_time": "2024-10-12T14:44:27.330Z"
      },
      {
        "duration": 2427,
        "start_time": "2024-10-12T14:44:27.429Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T14:44:29.857Z"
      },
      {
        "duration": 349,
        "start_time": "2024-10-12T14:48:45.063Z"
      },
      {
        "duration": 109,
        "start_time": "2024-10-12T14:48:49.266Z"
      },
      {
        "duration": 51,
        "start_time": "2024-10-12T14:48:52.915Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T14:49:30.413Z"
      },
      {
        "duration": 42,
        "start_time": "2024-10-12T14:49:33.059Z"
      },
      {
        "duration": 133,
        "start_time": "2024-10-12T14:52:47.038Z"
      },
      {
        "duration": 314,
        "start_time": "2024-10-12T14:54:16.370Z"
      },
      {
        "duration": 720197,
        "start_time": "2024-10-12T14:55:07.802Z"
      },
      {
        "duration": 267,
        "start_time": "2024-10-12T15:11:19.542Z"
      },
      {
        "duration": 720178,
        "start_time": "2024-10-12T15:12:13.029Z"
      },
      {
        "duration": 1096,
        "start_time": "2024-10-12T15:27:02.506Z"
      },
      {
        "duration": 2665,
        "start_time": "2024-10-12T15:27:16.731Z"
      },
      {
        "duration": 4613,
        "start_time": "2024-10-12T15:27:51.532Z"
      },
      {
        "duration": 5233,
        "start_time": "2024-10-12T15:28:04.667Z"
      },
      {
        "duration": 70808,
        "start_time": "2024-10-12T15:28:17.750Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T15:29:28.620Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T15:29:52.516Z"
      },
      {
        "duration": 17,
        "start_time": "2024-10-12T15:29:55.082Z"
      },
      {
        "duration": 127,
        "start_time": "2024-10-12T15:30:04.694Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T15:30:08.541Z"
      },
      {
        "duration": 1648129,
        "start_time": "2024-10-12T15:30:11.914Z"
      },
      {
        "duration": 1999,
        "start_time": "2024-10-12T15:57:55.433Z"
      },
      {
        "duration": 1937,
        "start_time": "2024-10-12T15:58:52.864Z"
      },
      {
        "duration": 48,
        "start_time": "2024-10-12T15:59:26.842Z"
      },
      {
        "duration": 1405,
        "start_time": "2024-10-12T15:59:57.594Z"
      },
      {
        "duration": 6325,
        "start_time": "2024-10-12T15:59:59.002Z"
      },
      {
        "duration": 2545,
        "start_time": "2024-10-12T16:00:05.328Z"
      },
      {
        "duration": 2447,
        "start_time": "2024-10-12T16:00:07.875Z"
      },
      {
        "duration": 6727,
        "start_time": "2024-10-12T16:00:10.324Z"
      },
      {
        "duration": 3707,
        "start_time": "2024-10-12T16:00:17.053Z"
      },
      {
        "duration": 3,
        "start_time": "2024-10-12T16:00:20.762Z"
      },
      {
        "duration": 110,
        "start_time": "2024-10-12T16:00:20.766Z"
      },
      {
        "duration": 304,
        "start_time": "2024-10-12T16:00:20.877Z"
      },
      {
        "duration": 213,
        "start_time": "2024-10-12T16:00:21.184Z"
      },
      {
        "duration": 3039,
        "start_time": "2024-10-12T16:00:21.398Z"
      },
      {
        "duration": 69,
        "start_time": "2024-10-12T16:00:24.439Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T16:00:24.509Z"
      },
      {
        "duration": 3433,
        "start_time": "2024-10-12T16:00:24.517Z"
      },
      {
        "duration": 7,
        "start_time": "2024-10-12T16:00:27.951Z"
      },
      {
        "duration": 3418,
        "start_time": "2024-10-12T16:00:27.959Z"
      },
      {
        "duration": 231,
        "start_time": "2024-10-12T16:00:31.378Z"
      },
      {
        "duration": 204,
        "start_time": "2024-10-12T16:00:31.610Z"
      },
      {
        "duration": 6,
        "start_time": "2024-10-12T16:00:31.820Z"
      },
      {
        "duration": 99,
        "start_time": "2024-10-12T16:00:31.828Z"
      },
      {
        "duration": 84,
        "start_time": "2024-10-12T16:00:31.929Z"
      },
      {
        "duration": 4515,
        "start_time": "2024-10-12T16:00:32.015Z"
      },
      {
        "duration": 71,
        "start_time": "2024-10-12T16:00:36.531Z"
      },
      {
        "duration": 1067,
        "start_time": "2024-10-12T16:00:36.603Z"
      },
      {
        "duration": 2728,
        "start_time": "2024-10-12T16:00:37.672Z"
      },
      {
        "duration": 8,
        "start_time": "2024-10-12T16:00:40.401Z"
      },
      {
        "duration": 88,
        "start_time": "2024-10-12T16:00:40.411Z"
      },
      {
        "duration": 1593,
        "start_time": "2024-10-12T16:00:40.502Z"
      },
      {
        "duration": 11,
        "start_time": "2024-10-12T16:00:42.096Z"
      },
      {
        "duration": 82,
        "start_time": "2024-10-12T16:00:42.109Z"
      },
      {
        "duration": 2560,
        "start_time": "2024-10-12T16:00:42.193Z"
      },
      {
        "duration": 4,
        "start_time": "2024-10-12T16:00:44.754Z"
      },
      {
        "duration": 721053,
        "start_time": "2024-10-12T16:00:44.760Z"
      },
      {
        "duration": 1124,
        "start_time": "2024-10-12T16:12:45.815Z"
      },
      {
        "duration": 1305,
        "start_time": "2024-10-12T16:12:46.941Z"
      },
      {
        "duration": 6174,
        "start_time": "2024-10-12T16:12:48.248Z"
      },
      {
        "duration": 4904,
        "start_time": "2024-10-12T16:12:54.424Z"
      },
      {
        "duration": 75598,
        "start_time": "2024-10-12T16:12:59.330Z"
      },
      {
        "duration": 1618,
        "start_time": "2024-10-12T16:14:14.929Z"
      },
      {
        "duration": 16,
        "start_time": "2024-10-12T16:14:16.549Z"
      },
      {
        "duration": 1701068,
        "start_time": "2024-10-12T16:14:16.567Z"
      },
      {
        "duration": 2048,
        "start_time": "2024-10-12T16:42:37.638Z"
      },
      {
        "duration": 46,
        "start_time": "2024-10-12T16:46:04.248Z"
      },
      {
        "duration": 104,
        "start_time": "2024-10-15T08:16:01.762Z"
      }
    ],
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Содержание",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "165px"
      },
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}